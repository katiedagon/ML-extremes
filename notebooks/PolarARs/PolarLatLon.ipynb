{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41157662-0d9e-4b44-a9d2-38fb15f98dda",
   "metadata": {},
   "source": [
    "# Convert Lat/Lon Files to Polar Stereographic Files\n",
    "\n",
    "This notebook provides a workflow for converting lat/lon netcdf files to polar stereographic coordinates. Then, we combine the polar stereographic underlying data files and mask files in order to use as input to cgnet. To get the data into the proper format, we needed to do the following:\n",
    " - utilize a template file as a base for modifying netcdf files of the existing mask arrays. The template file includes all proper GIS attributes.\n",
    " - python script reads the Geotiff file and creates a netcdf file with the x,y,lat,lon coordinate dimensions and variables\n",
    "\n",
    "Authors:\n",
    "--------\n",
    " - John Truesdale\n",
    " - Teagan King\n",
    "\n",
    "Prerequisites:\n",
    "--------------\n",
    "* Create geoTiff file (see instructions below)\n",
    "* Create SCRIP File\n",
    "* Generate remapped IVT/TMQ/etc underlying data (see instructions below)\n",
    "* Generate template files for NH & SH (see instructions below)\n",
    "* regrid TMQ/IVT/PSL/PR data from lat/lon to polar (see instructions below)\n",
    "* rename regridded files (see instructions below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1271ca6c-ebe4-4ac6-b348-67429dab4c41",
   "metadata": {},
   "source": [
    "Using QGIS to generate a GeoTIFF file for our polar jpgs:\n",
    "---------------------------------------------------------\n",
    " - Download [QGIS](https://qgis.org/)\n",
    " - Download a background North Polar Stereographic GeoTIFF file, eg from Polar Geospatial Center (PGC) Map Catalog \n",
    " - Open up QGIS, drag the background North Polar Stereographic GeoTIFF file into a new project, and ensure that the project coordinates match the imported image's coordinates (and are the same coordinates that you want to use in the georeferencing)\n",
    " - Assign a standard stereographic coordinate by using the Georeferencer tool in QGIS. Add the Polar jpg image used in climate contours into the georeferencer frame, and mark clear points (eg, islands or peninsulas or other landmarks) on the jpg and their corresponding points on the background North Polar Stereographic GeoTIFF.\n",
    " - When georeferencing, be sure to allow transforms rather than just linear adjustments.\n",
    " - Save information in a GeoTiff file by exporting the new layer.\n",
    "\n",
    "A few other notes from John:\n",
    "----------------------------\n",
    " - The polar projection jpegs on which the climatenet masks are drawn were created from python matplotlib and you can grab the coordinate information from matplotlib; this was checked with QGIS\n",
    " - Because the LLNL polar jpegs are a projected coordinate, the underlying unit in a stereographic projection is meters.  The x and y variables on the GeoTiff and converted netcdf file contain meter offsets of every pixel (row,col) of the ar_mask array with respect to one of the standard south pole stereographic coordinate systems.\n",
    " -   When you look at the square projected polar image you see that the longitude lines converge at the pole and latitudes are a set of nested circles.  When you are describing this grid in lat/lon coordinates it is known as a curvilinear grid where each pixel (array location) requires a unique lat/lon pair to specify its position on a regular grid. A straight line along any row or column of the jpeg raster (or ar_mask array) will intersect different lat lon values for every pixel.  The coordinate information for our rectangular ar_mask array therefore contains lat and lon variables that are two dimensions and describe the entire grid of 1152x1152 points with unique lat/lon values for each pixel (array location) of ar_mask. The standard netcdf way of denoting a curvilinear coordinate is by creating the dimensions that define the size of the ar_mask array (x,y), adding lat/lon variables that are each dimensioned (x,y) containing the lat/lon coordinates of that point, and finally adding metadata to the ar_mask array noting that the coordinates for this variable are not the dimension variables x,y but the lat/lon variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b27c606-38cb-492c-a4d8-f9ef05b2d915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from rasterio.warp import transform\n",
    "import urllib.request\n",
    "import xarray as xr\n",
    "import glob\n",
    "from netCDF4 import date2num\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import cftime\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321daf7f-ce7f-4fba-a2d4-56ac4ab905da",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate template file for SH & NH (only needs to be done once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d231450c-635f-4340-8e10-51e781b9baf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # TEMPLATE FILE FOR SH\n",
    "\n",
    "# # Read the data\n",
    "# input_path = '/glade/work/tking/cgnet/polar_regridding/data-2003-04-29-02-0-copy-sav1.tif'\n",
    "\n",
    "# da = xr.open_rasterio(input_path)\n",
    "# yval=da['y']\n",
    "# ryval=np.flip(yval)\n",
    "# # Compute the lon/lat coordinates with rasterio.warp.transform\n",
    "# ny, nx = len(da['y']), len(da['x'])\n",
    "# x, y = np.meshgrid(da['x'], ryval)\n",
    "# # Rasterio works with 1D arrays\n",
    "# lon, lat = transform(da.crs, {'init': 'EPSG:4326'},\n",
    "#                      x.flatten(), y.flatten())\n",
    "# lon = np.asarray(lon).reshape((ny, nx))\n",
    "# lat = np.asarray(lat).reshape((ny, nx))\n",
    "# da.coords['lon'] = (('y', 'x'), lon)\n",
    "# da.coords['lat'] = (('y', 'x'), lat)\n",
    "\n",
    "# da.to_netcdf(path='/glade/work/tking/cgnet/polar_regridding/data-2003-04-29-02-0-sav1-rev-latlon.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "024c25a9-ed4c-455e-a3ba-8dcc09b48885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEMPLATE FILE FOR NH\n",
    "\n",
    "# # Read the data\n",
    "# # input_path = '/glade/work/tking/cgnet/polar_regridding/NH.tif' #nh-rev-latlon.nc\n",
    "# input_path = '/glade/derecho/scratch/tking/cgnet/NPS/rendered.tif'\n",
    "\n",
    "# da = xr.open_rasterio(input_path) # may need to change to rioxarray.open_rasterio soon\n",
    "# yval=da['y']\n",
    "# ryval=np.flip(yval)\n",
    "# # Compute the lon/lat coordinates with rasterio.warp.transform\n",
    "# ny, nx = len(da['y']), len(da['x'])\n",
    "# x, y = np.meshgrid(da['x'], ryval)\n",
    "# # Rasterio works with 1D arrays\n",
    "# lon, lat = transform(da.crs, {'init': 'EPSG:4326'},\n",
    "#                      x.flatten(), y.flatten())\n",
    "# lon = np.asarray(lon).reshape((ny, nx))\n",
    "# lat = np.asarray(lat).reshape((ny, nx))\n",
    "# da.coords['lon'] = (('y', 'x'), lon)\n",
    "# da.coords['lat'] = (('y', 'x'), lat)\n",
    "\n",
    "# da.to_netcdf(path='/glade/work/tking/cgnet/polar_regridding/nh-rev-latlon_2.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d343dcca-82a8-4400-b39b-536cc612c5a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## set up dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f031f803-5c94-4e14-aff7-43be24698ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dask\n",
    "import dask\n",
    "\n",
    "# Use dask jobqueue\n",
    "from dask_jobqueue import PBSCluster\n",
    "\n",
    "# Import a client\n",
    "from dask.distributed import Client\n",
    "\n",
    "# Setup your PBSCluster\n",
    "nmem='25GB' # specify memory here so it duplicates below\n",
    "cluster = PBSCluster(\n",
    "    cores=1, # The number of cores you want\n",
    "    memory=nmem, # Amount of memory\n",
    "    processes=1, # How many processes\n",
    "    queue='casper', # The type of queue to utilize (/glade/u/apps/dav/opt/usr/bin/execcasper)\n",
    "    local_directory='/glade/derecho/scratch/$USER/local_dask', # Use your local directory\n",
    "    resource_spec='select=1:ncpus=1:mem='+nmem, # Specify resources\n",
    "    account='P93300313', # Input your project ID here, previously this was known as 'project', now is 'account'\n",
    "    walltime='08:00:00', # Amount of wall time\n",
    "    # interface='ib0', # Interface to use\n",
    ")\n",
    "\n",
    "# Scale up\n",
    "cluster.scale(10)\n",
    "\n",
    "# Change your url to the dask dashboard so you can see it\n",
    "dask.config.set({'distributed.dashboard.link':'https://jupyterhub.hpc.ucar.edu/stable/user/{USER}/proxy/{port}/status'})\n",
    "\n",
    "# Setup your client\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f69bfb96-a4d8-45b2-a457-732854f3824b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-72211184-295b-11f0-ab5f-3cecef1b11e8</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> dask_jobqueue.PBSCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"https://jupyterhub.hpc.ucar.edu/stable/user/tking/proxy/8787/status\" target=\"_blank\">https://jupyterhub.hpc.ucar.edu/stable/user/tking/proxy/8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">PBSCluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">ebf51b94</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"https://jupyterhub.hpc.ucar.edu/stable/user/tking/proxy/8787/status\" target=\"_blank\">https://jupyterhub.hpc.ucar.edu/stable/user/tking/proxy/8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 0\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 0\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 0 B\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-5b20e3a8-ac98-4e01-981a-cd8887db6247</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://128.117.208.98:40613\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"https://jupyterhub.hpc.ucar.edu/stable/user/tking/proxy/8787/status\" target=\"_blank\">https://jupyterhub.hpc.ucar.edu/stable/user/tking/proxy/8787/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 0 B\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tcp://128.117.208.98:40613' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e59a9d-b213-4494-af8b-2fec61683c25",
   "metadata": {
    "tags": []
   },
   "source": [
    "## define dictionary of file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d91b073-f069-481d-8f67-582234153ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmq_dict = {2000: \"prw_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200001010000-200012312100.nc\",\n",
    "            2001: \"prw_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200101010000-200112312100.nc\",\n",
    "            2002: \"prw_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200201010000-200212312100.nc\",\n",
    "            2003: \"prw_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200301010000-200312312100.nc\",\n",
    "            2004: \"prw_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200401010000-200412312100.nc\"}\n",
    "\n",
    "ivt_dict = {2000: \"windhusavi_3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200001-200012.nc\",\n",
    "            2001: \"windhusavi_3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200101-200112.nc\",\n",
    "            2002: \"windhusavi_3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200201-200212.nc\",\n",
    "            2003: \"windhusavi_3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200301-200312.nc\",\n",
    "            2004: \"windhusavi_3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200401-200412.nc\"}\n",
    "\n",
    "psl_dict = {2000: \"psl_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200001010000-200012312100.nc\",\n",
    "            2001: \"psl_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200101010000-200112312100.nc\",\n",
    "            2002: \"psl_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200201010000-200212312100.nc\",\n",
    "            2003: \"psl_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200301010000-200312312100.nc\",\n",
    "            2004: \"psl_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200401010000-200412312100.nc\"}\n",
    "\n",
    "pr_dict = {2000: \"pr_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200001010000-200012312359.nc\",\n",
    "           2001: \"pr_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200101010000-200112312359.nc\",\n",
    "           2002: \"pr_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200201010000-200212312359.nc\",\n",
    "           2003: \"pr_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200301010000-200312312359.nc\",\n",
    "           2004: \"pr_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200401010000-200412312359.nc\",\n",
    "           2005: \"pr_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200501010000-200512052359.nc\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c2fbc3-0d3e-4892-8b7a-81bb1234bb61",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Change to 2-dimensional lat/lon before regridding, then regrid using this section below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93084bb4-aee5-447e-9a68-8cc4f9d559e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # only do once-- and I think we used the 2dlatlon directory in the end?\n",
    "\n",
    "# for var in ['ivt']: # pr , 'psl', 'tmq'\n",
    "#     if var=='pr':\n",
    "#         dictionary = pr_dict\n",
    "#     elif var=='psl':\n",
    "#         dictionary = psl_dict\n",
    "#     elif var=='ivt':\n",
    "#         dictionary = ivt_dict\n",
    "#     elif var=='tmq':\n",
    "#         dictionary = tmq_dict\n",
    "#     for year in [2000]: #2001,2002,2003,2004]:\n",
    "#         ds_before_regrid = xr.open_dataset('/glade/derecho/scratch/tking/cgnet/high_lat_QC/from_nersc/2dlatlon/{}'.format(dictionary[year]))\n",
    "#         ds_before_regrid\n",
    "#         # we want this to be two dimensional lat/lon instead of 1\n",
    "\n",
    "#         # mesh is one useful tool but could do by hand\n",
    "#         # duplicate lat/lon array for lon/lat number of times\n",
    "#         # in order to have lat (y,x) and lon (y,x)\n",
    "\n",
    "#         # y is lat\n",
    "#         # x is lon\n",
    "#         # dimensions should be time, x, y, \n",
    "#         # follow example here: https://xesmf.readthedocs.io/en/latest/notebooks/Curvilinear_grid.html\n",
    "#         y_len = ds_before_regrid.lon.shape[0]\n",
    "#         x_len = ds_before_regrid.lat.shape[0]\n",
    "\n",
    "#         ds_before_regrid['lat_val'] = (('y','x'), np.tile(ds_before_regrid.lat, (y_len,1)))\n",
    "#         ds_before_regrid['lon_val'] = (('y','x'), np.transpose(np.tile(ds_before_regrid.lon, (x_len,1))))\n",
    "\n",
    "#         if var = 'ivt':\n",
    "#             ds_before_regrid[var] = ds_before_regrid.windhusavi.swap_dims({'lat':'x','lon':'y'})\n",
    "#         elif var = 'psl':\n",
    "#             ds_before_regrid[var] = ds_before_regrid.psl.swap_dims({'lat':'x','lon':'y'})\n",
    "#         elif var = 'tmq':\n",
    "#             ds_before_regrid[var] = ds_before_regrid.prw.swap_dims({'lat':'x','lon':'y'})\n",
    "#         elif var = 'pr':\n",
    "#             ds_before_regrid[var] = ds_before_regrid.pr.swap_dims({'lat':'x','lon':'y'})\n",
    "\n",
    "#         ds_before_regrid = ds_before_regrid.drop_dims('lat')\n",
    "#         ds_before_regrid = ds_before_regrid.drop_dims('lon')\n",
    "\n",
    "#         ds_before_regrid.to_netcdf('/glade/derecho/scratch/tking/cgnet/high_lat_QC/from_nersc/updated_latlon/{}'.format(dictionary[year]))\n",
    "#         print(\"done with {} year\".format(var), str(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c8233a-92cb-43db-8ce2-bc000a73d49c",
   "metadata": {},
   "source": [
    "# Create SCRIP file\n",
    "\n",
    "See [details on using curvilinear_to_SCRIP](https://www.ncl.ucar.edu/Document/Functions/ESMF/curvilinear_to_SCRIP.shtml) (example script pasted below)\n",
    "\n",
    "OR \n",
    "\n",
    "[Use ESMF_regrid](https://www.ncl.ucar.edu/Document/Functions/ESMF/ESMF_regrid.shtml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d7093f1-716a-41ee-b66d-d7d4d76d2c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curvilinear_to_SCRIP.ncl\n",
    "# ------\n",
    "# load \"$NCARG_ROOT/lib/ncarg/nclscripts/esmf/ESMF_regridding.ncl\"\n",
    "\n",
    "# begin\n",
    "# ;---Interpolation methods\n",
    "#     methods      = (/\"bilinear\",\"patch\",\"conserve\"/)\n",
    "\n",
    "# ;---Input file\n",
    "#     srcFileName  = \"nh-rev-latlon.nc\"\n",
    "\n",
    "# ;---Output (and input) files\n",
    "#     srcGridName  = \"np_stereographic_SCRIP.nc\"\n",
    "#     dstGridName  = \"nh_dst_SCRIP.nc\"\n",
    "\n",
    "# ;---Get data and lat/lon grid from CMIP5 Grid\n",
    "#     sfile       = addfile(srcFileName,\"r\")\n",
    "#     lat2d = sfile->lat\n",
    "#     lon2d = sfile->lon\n",
    "#     latlon_dims = dimsizes(lon2d)\n",
    "#     Opt         = True\n",
    "#     curvilinear_to_SCRIP(srcGridName,lat2d,lon2d, Opt)\n",
    "# end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ddf3c6-b2f5-4878-8647-31afac57cb76",
   "metadata": {
    "tags": []
   },
   "source": [
    "## regrid TMQ/IVT/PSL/PR data from lat/lon to polar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a689f456-8929-46e8-97b2-0a63de555a0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SH SCRIPT\n",
    "\n",
    "# module load gnu/9.1.0\n",
    "# module load esmf_libs/8.0.0\n",
    "# module load esmf-8.0.0-ncdfio-mpi-O\n",
    "# module load nco/4.7.9\n",
    "\n",
    "# set srcgrid=f09\n",
    "# set dstgrid=sp_stereo\n",
    "# set srcgridfile=/glade/p/cesmdata/inputdata/share/scripgrids/fv0.23x0.31_141008.nc\n",
    "# set dstgridfile=/glade/u/home/jet/sp_stereographic_SCRIP.nc\n",
    "# set srcinitfile=prw_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200001010000-200012312100.nc\n",
    "# set dstinitfile=polar_tmq/prw_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200001010000-200012312100_polar_nearest.nc\n",
    "\n",
    "# create the map file\n",
    "# ESMF_RegridWeightGen --ignore_unmapped --src_regional -m neareststod -w map_${srcgrid}_to_${dstgrid}_near.nc -s ${srcgridfile} -d ${dstgridfile}\n",
    "\n",
    "# submit ncremap scripts below in batch scripts, see example at\n",
    "#     /glade/derecho/scratch/tking/cgnet/high_lat_QC/from_nersc/2dlatlon/sh_scripts/remap_script and batch_remap.sh\n",
    "\n",
    "# the above batch script uses the mapfile to remap srcinitfile to dstinitfile as shown below:\n",
    "# ncremap -m ./map_${srcgrid}_to_${dstgrid}_near.nc -i ${srcinitfile} -o ${dstinitfile}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bcb116-68b8-4e20-b368-579372a4ceca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # # NH SCRIPT (tcsh)\n",
    "\n",
    "# module load gnu/9.1.0\n",
    "# module load esmf_libs/8.0.0\n",
    "# module load esmf-8.0.0-ncdfio-mpi-O\n",
    "# module load nco/4.7.9\n",
    "\n",
    "# set srcgrid=f09\n",
    "# set dstgrid=np_stereo\n",
    "# set srcgridfile=/glade/p/cesmdata/inputdata/share/scripgrids/fv0.23x0.31_141008.nc\n",
    "# set dstgridfile=/glade/derecho/scratch/tking/cgnet/high_lat_QC/from_nersc/2dlatlon/nh_scripts/np_stereographic_SCRIP_2.nc\n",
    "# set srcinitfile=prw.nc\n",
    "# set dstinitfile=/glade/derecho/scratch/tking/cgnet/high_lat_QC/from_nersc/2dlatlon/nh_polar/tmq/prw_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200001010000-200012312100_polar_nearest.nc\n",
    "\n",
    "# # create the map file\n",
    "# # ESMF_RegridWeightGen --ignore_unmapped --src_regional -m neareststod -w map_${srcgrid}_to_${dstgrid}_near.nc -s ${srcgridfile} -d ${dstgridfile}\n",
    "# ESMF_RegridWeightGen --ignore_unmapped --src_regional -m neareststod -w map_fv0.23x0.31_to_${dstgrid}_near_2.nc -s ${srcgridfile} -d ${dstgridfile}\n",
    "\n",
    "\n",
    "# use the mapfile to remap srcinitfile to dstinitfile using batch scripts like those below which include commands such as \"ncremap -m ./map_${srcgrid}_to_${dstgrid}_near.nc -i ${srcinitfile} -o ${dstinitfile}\"\n",
    "# submit ncremap scripts in batch scripts with qsub, see example:\n",
    "#     /glade/derecho/scratch/tking/cgnet/high_lat_QC/from_nersc/2dlatlon/nh_scripts/remap_script and batch_remap.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c946758e-6194-443b-86ab-77aa474627fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Make North Polar Stereographic Blue Marble Image (I actually did this in QGIS in the end...)\n",
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import cartopy.crs as ccrs\n",
    "# from PIL import Image\n",
    "# from cartopy import img_transform\n",
    "\n",
    "# # Load the image\n",
    "# img = Image.open(\"/glade/work/tking/cgnet/ClimateNet/climatenet/bluemarble_fake/BM_for_NP.jpeg\")\n",
    "\n",
    "# # Define projections\n",
    "# src_proj = ccrs.PlateCarree()  # Equirectangular input\n",
    "# dst_proj = ccrs.NorthPolarStereo()  # North Polar Stereographic\n",
    "\n",
    "# # Create figure\n",
    "# fig = plt.figure(figsize=(8, 8))\n",
    "# ax = plt.axes(projection=dst_proj)\n",
    "\n",
    "# # Set extent for the North Polar Stereographic projection\n",
    "# ax.set_extent([-180, 180, 45, 90], crs=src_proj)  # Crop to Northern Hemisphere\n",
    "\n",
    "# # Reproject the image to the new projection using imshow\n",
    "# ax.imshow(img, origin=\"upper\", transform=src_proj, extent=[-180, 180, -90, 90])\n",
    "\n",
    "# # Save and show output\n",
    "# plt.savefig(\"/glade/work/tking/cgnet/ClimateNet/climatenet/bluemarble_fake/BM_NP.jpeg\", dpi=300, bbox_inches=\"tight\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f9f7636-6789-48c7-8348-e82ebee72b02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import cartopy.crs as ccrs\n",
    "# from PIL import Image\n",
    "# import cartopy.feature as cfeature\n",
    "\n",
    "# # Load the image\n",
    "# img = Image.open(\"/glade/work/tking/cgnet/ClimateNet/climatenet/bluemarble_fake/BM_for_NP.jpeg\")\n",
    "\n",
    "# # Define projections\n",
    "# src_proj = ccrs.PlateCarree()  # Equirectangular input\n",
    "# dst_proj = ccrs.NorthPolarStereo()  # North Polar Stereographic\n",
    "\n",
    "# # Create figure and axis with the dst_proj (polar projection)\n",
    "# fig = plt.figure(figsize=(8, 8))\n",
    "# ax = plt.axes(projection=dst_proj)\n",
    "\n",
    "# # Add geographic features to help visualize the projection\n",
    "# ax.add_feature(cfeature.LAND)\n",
    "# ax.add_feature(cfeature.OCEAN)\n",
    "\n",
    "# # Set extent for the North Polar Stereographic projection\n",
    "# # Adjust the extent to match the Northern Hemisphere\n",
    "# ax.set_extent([-180, 180, 45, 90], crs=src_proj)  # Crop to Northern Hemisphere\n",
    "\n",
    "# # Ensure that the image is transformed correctly\n",
    "# # Here we need to properly set the extent and use the projection of the image (src_proj)\n",
    "# ax.imshow(img, origin=\"upper\", transform=src_proj, extent=[-180, 180, -90, 90])\n",
    "\n",
    "# # Save and show output\n",
    "# plt.savefig(\"/glade/work/tking/cgnet/ClimateNet/climatenet/bluemarble_fake/BM_NP.jpeg\", dpi=300, bbox_inches=\"tight\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2ba3c5-f12e-4c61-9c95-366cf5f41fd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Rename and format regridded files for training and inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db87e669-0567-4347-b1a0-6f0266e786e1",
   "metadata": {},
   "source": [
    "For instance:\n",
    "\n",
    "1) Rename lat/lon dimensions so as not to be overwritten: `ncrename -d lon,x -d lat,y VAR/$FILENAME renamed/VAR/$FILENAME`\n",
    "2) For any files with variables that need renaming:\n",
    "\n",
    "    `ncrename -v prw,tmq $FILENAME`\n",
    "\n",
    "    `ncrename -v windhusavi,ivt $FILENAME`\n",
    "5) `ncks -6 $FILENAME` ? This converts to netcdf4 classic (maybe not necessary? I think this was also only done for one variable. Skipping in NH polar process...)\n",
    "\n",
    "Steps to include a file in new_latlon dir:\n",
    "6) Rename lon variable to longitude (ideally would be with this but it fails: `ncrename -v lon,longitude $FILENAME`) so use the following steps INSTEAD:\n",
    "`ncap2 -s 'longitude=lon' $FILENAME $FILENAME_tmp.nc`\n",
    "`ncks -C -x -v lon $FILENAME_tmp.nc $FILENAME_renamed.nc`\n",
    "7) Rename lat variable to latitude: `ncrename -v lat,latitude $FILENAME`\n",
    "8) Rename lat/lon dimensions back after not overwritten: `ncrename -d x,lon -d y,lat $FILENAME` (maybe this was skipped psl?)\n",
    "\n",
    "9) flip horizontally with: `python /glade/derecho/scratch/tking/cgnet/high_lat_QC/from_nersc/2dlatlon/nh_polar/renamed/ivt/flip_image.py <INPUT FILE> <OUTPUT_FILE>`\n",
    "\n",
    "10) any cropping necessary: /glade/derecho/scratch/tking/cgnet/high_lat_QC/from_nersc/2dlatlon/nh_polar/renamed/ivt/zoom.py (use 450)\n",
    "\n",
    "11) Split files:\n",
    "   `cdo splitsel,1 $FILENAME YEAR_split_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac0d1fd4-f6aa-48d0-a53d-7717a02fdd9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt;\n",
       "Dimensions:    (lat: 1152, lon: 1152, height: 1, nvertices: 4, time: 2920,\n",
       "                nb2: 2)\n",
       "Coordinates:\n",
       "  * height     (height) float32 0.0\n",
       "  * time       (time) object 2000-01-01 01:30:00 ... 1979-01-01 00:00:00.000255\n",
       "Dimensions without coordinates: lat, lon, nvertices, nb2\n",
       "Data variables:\n",
       "    area       (lat, lon) float64 ...\n",
       "    latitude   (lat, lon) float64 ...\n",
       "    lat_bnds   (lat, lon, nvertices) float64 ...\n",
       "    lon_bnds   (lat, lon, nvertices) float64 ...\n",
       "    longitude  (lat, lon) float64 ...\n",
       "    pr         (time, height, lat, lon) float32 ...\n",
       "    time_bnds  (time, nb2) object ...\n",
       "Attributes: (12/28)\n",
       "    history:                   Sat May  3 13:54:59 2025: ncrename -v lat,lati...\n",
       "    forcing:                   SST=sst_HadOIBl_bc_0.23x0.31_1870_2008_c091020...\n",
       "    institute_run_id:          cam5_1_amip_run2\n",
       "    CDO:                       Climate Data Operators version 1.6.5.2 (http:/...\n",
       "    institution:               Lawrence Berkeley National Laboratory, Berkele...\n",
       "    institute_id:              LBNL\n",
       "    ...                        ...\n",
       "    remap_script:              ncremap\n",
       "    remap_hostname:            casper-login1\n",
       "    remap_version:             5.3.1\n",
       "    NCO:                       netCDF Operators version 5.3.1 (Homepage = htt...\n",
       "    map_file:                  nh_scripts/map_fv0.23x0.31_to_np_stereo_near_4.nc\n",
       "    input_file:                pr/pr_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0...</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-fd76444f-ce44-47e3-a5c3-e0f749e91f8d' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-fd76444f-ce44-47e3-a5c3-e0f749e91f8d' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span>lat</span>: 1152</li><li><span>lon</span>: 1152</li><li><span class='xr-has-index'>height</span>: 1</li><li><span>nvertices</span>: 4</li><li><span class='xr-has-index'>time</span>: 2920</li><li><span>nb2</span>: 2</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-d4b98273-ad32-4808-aa01-acadb5546679' class='xr-section-summary-in' type='checkbox'  checked><label for='section-d4b98273-ad32-4808-aa01-acadb5546679' class='xr-section-summary' >Coordinates: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>height</span></div><div class='xr-var-dims'>(height)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>0.0</div><input id='attrs-9694387f-266a-402b-9acb-a3c8fae87966' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-9694387f-266a-402b-9acb-a3c8fae87966' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-82a03d2d-d95d-41db-9321-eb4f99f0777b' class='xr-var-data-in' type='checkbox'><label for='data-82a03d2d-d95d-41db-9321-eb4f99f0777b' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>axis :</span></dt><dd>Z</dd><dt><span>long_name :</span></dt><dd>height</dd><dt><span>positive :</span></dt><dd>up</dd><dt><span>standard_name :</span></dt><dd>height</dd><dt><span>units :</span></dt><dd>m</dd></dl></div><div class='xr-var-data'><pre>array([0.], dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>object</div><div class='xr-var-preview xr-preview'>2000-01-01 01:30:00 ... 1979-01-...</div><input id='attrs-ff48a99b-13af-49b9-acfc-80a75def79f9' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-ff48a99b-13af-49b9-acfc-80a75def79f9' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-ad2af66a-a784-43c3-b8bc-15815f989c0f' class='xr-var-data-in' type='checkbox'><label for='data-ad2af66a-a784-43c3-b8bc-15815f989c0f' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>axis :</span></dt><dd>T</dd><dt><span>bounds :</span></dt><dd>time_bnds</dd><dt><span>long_name :</span></dt><dd>time</dd><dt><span>standard_name :</span></dt><dd>time</dd></dl></div><div class='xr-var-data'><pre>array([cftime.DatetimeNoLeap(2000, 1, 1, 1, 30, 0, 0, has_year_zero=True),\n",
       "       cftime.DatetimeNoLeap(2000, 1, 1, 4, 30, 0, 0, has_year_zero=True),\n",
       "       cftime.DatetimeNoLeap(2000, 1, 1, 7, 30, 0, 0, has_year_zero=True), ...,\n",
       "       cftime.DatetimeNoLeap(1979, 1, 1, 0, 0, 0, 0, has_year_zero=True),\n",
       "       cftime.DatetimeNoLeap(1979, 1, 1, 0, 0, 0, 33, has_year_zero=True),\n",
       "       cftime.DatetimeNoLeap(1979, 1, 1, 0, 0, 0, 255, has_year_zero=True)],\n",
       "      dtype=object)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-f86b780b-6fcf-423c-90f5-3cc7ddace7aa' class='xr-section-summary-in' type='checkbox'  checked><label for='section-f86b780b-6fcf-423c-90f5-3cc7ddace7aa' class='xr-section-summary' >Data variables: <span>(7)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>area</span></div><div class='xr-var-dims'>(lat, lon)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-62ebf78b-3bab-407f-88dc-2245d66d3f9b' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-62ebf78b-3bab-407f-88dc-2245d66d3f9b' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-fb6c7d3a-5b34-461a-a5be-ae42b162c636' class='xr-var-data-in' type='checkbox'><label for='data-fb6c7d3a-5b34-461a-a5be-ae42b162c636' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>Solid angle subtended by gridcell</dd><dt><span>standard_name :</span></dt><dd>solid_angle</dd><dt><span>units :</span></dt><dd>steradian</dd><dt><span>coordinates :</span></dt><dd>lat lon</dd><dt><span>cell_mathods :</span></dt><dd>lat, lon: sum</dd></dl></div><div class='xr-var-data'><pre>[1327104 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>latitude</span></div><div class='xr-var-dims'>(lat, lon)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-177c4574-5cac-4533-81e4-1528ac9f1332' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-177c4574-5cac-4533-81e4-1528ac9f1332' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-4dfc0424-4bac-400c-b04b-93624eedd07e' class='xr-var-data-in' type='checkbox'><label for='data-4dfc0424-4bac-400c-b04b-93624eedd07e' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>Latitude of Grid Cell Centers</dd><dt><span>standard_name :</span></dt><dd>latitude</dd><dt><span>units :</span></dt><dd>degrees_north</dd><dt><span>valid_min :</span></dt><dd>-90.0</dd><dt><span>valid_max :</span></dt><dd>90.0</dd><dt><span>bounds :</span></dt><dd>lat_bnds</dd></dl></div><div class='xr-var-data'><pre>[1327104 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>lat_bnds</span></div><div class='xr-var-dims'>(lat, lon, nvertices)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-8d222aab-54d1-4962-ac8d-6ec0f14efa85' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-8d222aab-54d1-4962-ac8d-6ec0f14efa85' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-a1b9947d-6373-4952-a214-cef9eefd3eae' class='xr-var-data-in' type='checkbox'><label for='data-a1b9947d-6373-4952-a214-cef9eefd3eae' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>Gridcell latitude vertices</dd></dl></div><div class='xr-var-data'><pre>[5308416 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>lon_bnds</span></div><div class='xr-var-dims'>(lat, lon, nvertices)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-a2f45748-23c9-4a6e-a2fc-b20b2b33c92d' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-a2f45748-23c9-4a6e-a2fc-b20b2b33c92d' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-8a20bfc2-8ebe-460a-8e88-ff22c8e8ba33' class='xr-var-data-in' type='checkbox'><label for='data-8a20bfc2-8ebe-460a-8e88-ff22c8e8ba33' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>Gridcell longitude vertices</dd></dl></div><div class='xr-var-data'><pre>[5308416 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>longitude</span></div><div class='xr-var-dims'>(lat, lon)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-27f8856f-3bae-4f80-a296-d8ec5a58532a' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-27f8856f-3bae-4f80-a296-d8ec5a58532a' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-e76f09eb-2f9d-4861-b193-b5c6713f066a' class='xr-var-data-in' type='checkbox'><label for='data-e76f09eb-2f9d-4861-b193-b5c6713f066a' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>bounds :</span></dt><dd>lon_bnds</dd><dt><span>long_name :</span></dt><dd>Longitude of Grid Cell Centers</dd><dt><span>modulo :</span></dt><dd>360.0</dd><dt><span>standard_name :</span></dt><dd>longitude</dd><dt><span>topology :</span></dt><dd>circular</dd><dt><span>units :</span></dt><dd>degrees_east</dd><dt><span>valid_max :</span></dt><dd>180.0</dd><dt><span>valid_min :</span></dt><dd>-180.0</dd></dl></div><div class='xr-var-data'><pre>[1327104 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>pr</span></div><div class='xr-var-dims'>(time, height, lat, lon)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-8bf90017-88ca-4a74-9a22-0125790b4157' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-8bf90017-88ca-4a74-9a22-0125790b4157' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-9b01cf42-145a-4d5d-adb2-10b296d94da4' class='xr-var-data-in' type='checkbox'><label for='data-9b01cf42-145a-4d5d-adb2-10b296d94da4' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>Precipitation</dd><dt><span>original_name :</span></dt><dd>PRECT</dd><dt><span>standard_name :</span></dt><dd>precipitation_flux</dd><dt><span>units :</span></dt><dd>kg m-2 s-1</dd><dt><span>coordinates :</span></dt><dd>lat lon</dd><dt><span>cell_measures :</span></dt><dd>area: area</dd></dl></div><div class='xr-var-data'><pre>[3875143680 values with dtype=float32]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>time_bnds</span></div><div class='xr-var-dims'>(time, nb2)</div><div class='xr-var-dtype'>object</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-0a78affe-5093-4d2d-9cb7-63904a71927f' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-0a78affe-5093-4d2d-9cb7-63904a71927f' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-d6ccc814-3762-47e7-9d08-44f472789217' class='xr-var-data-in' type='checkbox'><label for='data-d6ccc814-3762-47e7-9d08-44f472789217' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[5840 values with dtype=object]</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-b64687be-ff7f-4aa2-ab0d-c57ddb221719' class='xr-section-summary-in' type='checkbox'  ><label for='section-b64687be-ff7f-4aa2-ab0d-c57ddb221719' class='xr-section-summary' >Indexes: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>height</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-1d5c05fa-922a-47e6-8e2b-63f4f7910262' class='xr-index-data-in' type='checkbox'/><label for='index-1d5c05fa-922a-47e6-8e2b-63f4f7910262' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Float64Index([0.0], dtype=&#x27;float64&#x27;, name=&#x27;height&#x27;))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>time</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-36f5e771-560b-431e-8ae4-3be9d9abbcea' class='xr-index-data-in' type='checkbox'/><label for='index-36f5e771-560b-431e-8ae4-3be9d9abbcea' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(CFTimeIndex([2000-01-01 01:30:00, 2000-01-01 04:30:00, 2000-01-01 07:30:00,\n",
       "             2000-01-01 10:30:00, 2000-01-01 13:30:00, 2000-01-01 16:30:00,\n",
       "             2000-01-01 19:30:00, 2000-01-01 22:30:00, 2000-01-02 01:30:00,\n",
       "             2000-01-02 04:30:00,\n",
       "             ...\n",
       "             1979-01-01 00:00:00, 1979-01-01 00:00:00, 1979-01-01 00:00:00,\n",
       "             1979-01-01 00:00:00, 1979-01-01 00:00:00, 1979-01-01 00:00:00,\n",
       "             1979-01-01 00:00:00, 1979-01-01 00:00:00, 1979-01-01 00:00:00.000033,\n",
       "             1979-01-01 00:00:00.000255],\n",
       "            dtype=&#x27;object&#x27;, length=2920, calendar=&#x27;noleap&#x27;, freq=&#x27;None&#x27;))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-ec69adc5-9223-4ab7-ba6a-dc1ed6df7976' class='xr-section-summary-in' type='checkbox'  ><label for='section-ec69adc5-9223-4ab7-ba6a-dc1ed6df7976' class='xr-section-summary' >Attributes: <span>(28)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>history :</span></dt><dd>Sat May  3 13:54:59 2025: ncrename -v lat,latitude pr_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200001010000-200012312359.nc\n",
       "Sat May  3 13:51:55 2025: ncks -C -x -v lon pr_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200001010000-200012312359_tmp.nc new_latlon/pr_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200001010000-200012312359.nc\n",
       "Sat May  3 11:00:01 2025: ncap2 -s longitude=lon pr_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200001010000-200012312359.nc pr_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200001010000-200012312359_tmp.nc\n",
       "Fri May  2 11:40:51 2025: ncks -O -t 2 --no_tmp_fl --hdr_pad=10000 --gaa remap_script=ncremap --gaa remap_hostname=casper-login1 --gaa remap_version=5.3.1 --rgr lat_nm_out=lat --rgr lon_nm_out=lon --map_fl=nh_scripts/map_fv0.23x0.31_to_np_stereo_near_4.nc pr/pr_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200001010000-200012312359.nc nh_polar/pr/pr_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200001010000-200012312359.nc\n",
       "2017-03-03T13:07:25Z: Naming and attribute conventions updated with C20C_prepare_for_portal.sh v5.\n",
       " unknown: Original creation date</dd><dt><span>forcing :</span></dt><dd>SST=sst_HadOIBl_bc_0.23x0.31_1870_2008_c091020.nc; SIC=sst_HadOIBl_bc_0.23x0.31_1870_2008_c091020.nc; GHG=ghg_hist_1765-2005_c091218.nc; Aerosol=aero_1.9x2.5_L26_2000clim_c091112.nc, aerosoldep_monthly_1849-2006_1.9x2.5_c090803.nc, oxid_1.9x2.5_L26_1850-2005_c091123.nc (cyclical year-2000: sulf:SO4, bcar1:CB1, bcar2:CB2, ocar1:OC1, ocar2:OC2, sslt1:SSLT01, sslt2:SSLT02, sslt3:SSLT03, sslt4:SSLT04, dust1:DST01, dust2:DST02, dust3:DST03, dust4:DST04); Ozone=ozone_1.9x2.5_L26_1850-2005_c090803.nc; Land-use=surfdata_0.23x0.31_simyr1850_c100404.nc; Solar=SOLAR_SPECTRAL_Lean_1610-2008_annual_c090324.nc; Volcanic=CCSM4_volcanic_1850-2008_prototype1.nc</dd><dt><span>institute_run_id :</span></dt><dd>cam5_1_amip_run2</dd><dt><span>CDO :</span></dt><dd>Climate Data Operators version 1.6.5.2 (http://code.zmaw.de/projects/cdo)</dd><dt><span>institution :</span></dt><dd>Lawrence Berkeley National Laboratory, Berkeley, CA, USA</dd><dt><span>institute_id :</span></dt><dd>LBNL</dd><dt><span>experiment_family :</span></dt><dd>All-Hist</dd><dt><span>experiment :</span></dt><dd>est1</dd><dt><span>subexperiment :</span></dt><dd>v1-0</dd><dt><span>run_id :</span></dt><dd>run002</dd><dt><span>model_id :</span></dt><dd>CAM5.1-0.25degree</dd><dt><span>frequency :</span></dt><dd>3hr</dd><dt><span>contact :</span></dt><dd>mfwehner@lbl.gov</dd><dt><span>title :</span></dt><dd>CAM5.1 model at 0.3125x0.2344degree resolution</dd><dt><span>acknowledgement :</span></dt><dd>This simulation was performed under the CASCADE Science Focus Area funded by the U.S. Department of Energy, Office of Science, Office of Biological and Environmental Research&#x27;s Regional and Global Climate Modeling Program under contract number DE-AC02-05CH11231. It was performed on hopper.nersc.gov at the National Energy Research Supercomputer Center (NERSC), also supported by the Office of Science of the U.S. Department of Energy, under Contract No. DE-AC02-05CH11231.</dd><dt><span>parent_experiment_family :</span></dt><dd>N/A</dd><dt><span>parent_experiment :</span></dt><dd>N/A</dd><dt><span>parent_subexperiment :</span></dt><dd>N/A</dd><dt><span>parent_run_id :</span></dt><dd>N/A</dd><dt><span>project_id :</span></dt><dd>C20C+ Detection and Attribution Project</dd><dt><span>license :</span></dt><dd>Creative Commons License: http://creativecommons.org/licenses/by-nc-sa/2.0/</dd><dt><span>creation_date :</span></dt><dd>2017-03-03T13:07:25Z</dd><dt><span>remap_script :</span></dt><dd>ncremap</dd><dt><span>remap_hostname :</span></dt><dd>casper-login1</dd><dt><span>remap_version :</span></dt><dd>5.3.1</dd><dt><span>NCO :</span></dt><dd>netCDF Operators version 5.3.1 (Homepage = http://nco.sf.net, Code = http://github.com/nco/nco, Citation = 10.1016/j.envsoft.2008.03.004)</dd><dt><span>map_file :</span></dt><dd>nh_scripts/map_fv0.23x0.31_to_np_stereo_near_4.nc</dd><dt><span>input_file :</span></dt><dd>pr/pr_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200001010000-200012312359.nc</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:    (lat: 1152, lon: 1152, height: 1, nvertices: 4, time: 2920,\n",
       "                nb2: 2)\n",
       "Coordinates:\n",
       "  * height     (height) float32 0.0\n",
       "  * time       (time) object 2000-01-01 01:30:00 ... 1979-01-01 00:00:00.000255\n",
       "Dimensions without coordinates: lat, lon, nvertices, nb2\n",
       "Data variables:\n",
       "    area       (lat, lon) float64 ...\n",
       "    latitude   (lat, lon) float64 ...\n",
       "    lat_bnds   (lat, lon, nvertices) float64 ...\n",
       "    lon_bnds   (lat, lon, nvertices) float64 ...\n",
       "    longitude  (lat, lon) float64 ...\n",
       "    pr         (time, height, lat, lon) float32 ...\n",
       "    time_bnds  (time, nb2) object ...\n",
       "Attributes: (12/28)\n",
       "    history:                   Sat May  3 13:54:59 2025: ncrename -v lat,lati...\n",
       "    forcing:                   SST=sst_HadOIBl_bc_0.23x0.31_1870_2008_c091020...\n",
       "    institute_run_id:          cam5_1_amip_run2\n",
       "    CDO:                       Climate Data Operators version 1.6.5.2 (http:/...\n",
       "    institution:               Lawrence Berkeley National Laboratory, Berkele...\n",
       "    institute_id:              LBNL\n",
       "    ...                        ...\n",
       "    remap_script:              ncremap\n",
       "    remap_hostname:            casper-login1\n",
       "    remap_version:             5.3.1\n",
       "    NCO:                       netCDF Operators version 5.3.1 (Homepage = htt...\n",
       "    map_file:                  nh_scripts/map_fv0.23x0.31_to_np_stereo_near_4.nc\n",
       "    input_file:                pr/pr_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check a file:\n",
    "pr_ds = xr.open_dataset('/glade/derecho/scratch/tking/cgnet/high_lat_QC/from_nersc/2dlatlon/nh_polar/renamed/pr/new_latlon/pr_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200001010000-200012312359.nc')\n",
    "pr_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc325c77-d889-4a5f-ae6d-8fe5c2ad9e53",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate combined mask/underlying data files by creating netcdf from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13434f1-62c3-4439-a89c-49a0a30734c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### run the next cell if temp.nc (temporary file to fill) already exists -- SPECIFY ANTARCTIC OR ARCTIC!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48ab9c5e-4bd1-4047-8514-f572ce8fa4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm /glade/u/home/tking/work/cgnet/QA_xml/all_arctic_converted_masks/temp.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12cef01-3e47-42cc-95cb-0a0843bf9123",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Note on time adjustment\n",
    "We'll need to use the bug fix included in the below cells for the first two rounds of QC'd data; this issue has been fixed in the masks of following datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b365e3d0-5f24-4271-a9b1-adb4a3b06ce6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create temp file with correct attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16b96067-d93f-448e-9069-2200d8137bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET THESE THINGS:\n",
    "\n",
    "# is temp file for inference or not?\n",
    "inference=False\n",
    "region = 'arctic' # 'antarctic'\n",
    "\n",
    "round_val = 2  # change this value to indicate whether or not bug fix from round 1 is used\n",
    "year_val = 2004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3923a380-aa50-490b-b80a-90139416b44b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if region == 'antarctic':\n",
    "    temp_file = '/glade/work/tking/cgnet/polar_regridding/data-2003-04-29-02-0-sav1-rev-latlon.nc'\n",
    "    if not inference:\n",
    "        ncfile = Dataset(f'/glade/u/home/tking/work/cgnet/QA_xml/all_antarctic_converted_masks/{year_val}_round{round_val}.nc',mode='w',format='NETCDF4_CLASSIC') \n",
    "\n",
    "if region == 'arctic':\n",
    "    temp_file = '/glade/work/tking/cgnet/polar_regridding/nh.nc'\n",
    "    if not inference:\n",
    "        ncfile = Dataset(f'/glade/u/home/tking/work/cgnet/QA_xml/all_arctic_converted_masks/{year_val}_round{round_val}.nc',mode='w',format='NETCDF4_CLASSIC') \n",
    "    if inference:\n",
    "        ncfile = Dataset(f'/glade/u/home/tking/work/cgnet/QA_xml/all_arctic_converted_masks/inference.nc',mode='w',format='NETCDF4_CLASSIC') \n",
    "\n",
    "temp = xr.open_dataset(temp_file)\n",
    "\n",
    "# Create dimensions\n",
    "y_dim = ncfile.createDimension('y', 1152)        # vertical displacement axis\n",
    "x_dim = ncfile.createDimension('x', 1152)        # horizontal displacement axis\n",
    "time_dim = ncfile.createDimension('time', None)  # unlimited axis (can be appended to)\n",
    "sample_id_dim = ncfile.createDimension('sample_id', 6)\n",
    "\n",
    "# Include time variable and relevant attributes\n",
    "time = ncfile.createVariable('time', np.float64, ('time', ))\n",
    "time.units = 'hours since 1970-01-01'\n",
    "time.calendar = 'noleap'\n",
    "time.long_name = 'time'\n",
    "\n",
    "# Include date information\n",
    "date = ncfile.createVariable('date', np.float64, ('time', ))\n",
    "date.long_name = 'current date'\n",
    "datesec = ncfile.createVariable('datesec', np.float64, ('time', ))\n",
    "datesec.long_name = 'current seconds of current date'\n",
    "\n",
    "if not inference:\n",
    "    # Include ar_mask variable and relevant attributes\n",
    "    ar_mask = ncfile.createVariable('ar_mask', np.float64, ('time', 'sample_id', 'y', 'x'))\n",
    "    ar_mask.long_name = \"Atmospheric River Mask\"\n",
    "    ar_mask.standard_name = \"AR flag\"\n",
    "    ar_mask.flag_values = 0, 1\n",
    "    ar_mask.flag_meanings = \"Background Atmospheric_River\"\n",
    "\n",
    "# Include underlying data\n",
    "tmq = ncfile.createVariable('tmq', np.float64,('time', 'y', 'x'))\n",
    "ivt = ncfile.createVariable('ivt', np.float64,('time', 'y', 'x'))\n",
    "psl = ncfile.createVariable('psl', np.float64,('time', 'y', 'x'))\n",
    "pr = ncfile.createVariable('pr', np.float64,('time', 'y', 'x'))\n",
    "\n",
    "# include y, x, lat, & lon from temp file\n",
    "y = ncfile.createVariable('y', np.float64,('y'))\n",
    "y.long_name = 'vertical offset from pole'\n",
    "y.units = 'meters'\n",
    "\n",
    "x = ncfile.createVariable('x',np.float64,('x'))\n",
    "x.long_name = 'horizontal offset from pole'\n",
    "x.units = 'meters'\n",
    "\n",
    "lat = ncfile.createVariable('lat', np.float64,('y','x'))\n",
    "lat.units = 'degrees_north'\n",
    "lat.long_name = 'latitude'\n",
    "\n",
    "lon = ncfile.createVariable('lon', np.float64,('y','x'))\n",
    "lon.units = 'degrees_east'\n",
    "lon.long_name = 'longitude'\n",
    "\n",
    "# Add the y, x, lat, & lon data values to the netcdf file\n",
    "ncfile['y'][:] = temp.y\n",
    "ncfile['x'][:] = temp.x\n",
    "ncfile['lat'][:,:] = temp.lat\n",
    "ncfile['lon'][:,:] = temp.lon\n",
    "\n",
    "# Copy temp file metadata\n",
    "ivt.transform = temp.__xarray_dataarray_variable__.transform\n",
    "ivt.crs = temp.__xarray_dataarray_variable__.crs\n",
    "ivt.coordinates = 'lat lon'\n",
    "tmq.transform = temp.__xarray_dataarray_variable__.transform\n",
    "tmq.crs = temp.__xarray_dataarray_variable__.crs\n",
    "tmq.coordinates = 'lat lon'\n",
    "psl.transform = temp.__xarray_dataarray_variable__.transform\n",
    "psl.crs = temp.__xarray_dataarray_variable__.crs\n",
    "psl.coordinates = 'lat lon'\n",
    "pr.transform = temp.__xarray_dataarray_variable__.transform\n",
    "pr.crs = temp.__xarray_dataarray_variable__.crs\n",
    "pr.coordinates = 'lat lon'\n",
    "\n",
    "# Copy global metadata\n",
    "ncfile.transform = temp.__xarray_dataarray_variable__.transform\n",
    "ncfile.crs = temp.__xarray_dataarray_variable__.crs\n",
    "ncfile.res = temp.__xarray_dataarray_variable__.res\n",
    "ncfile.nodatavals = temp.__xarray_dataarray_variable__.nodatavals\n",
    "ncfile.scales = temp.__xarray_dataarray_variable__.scales\n",
    "ncfile.offsets = temp.__xarray_dataarray_variable__.offsets\n",
    "ncfile.AREA_OR_POINT = temp.__xarray_dataarray_variable__.AREA_OR_POINT\n",
    "ncfile.coordinates = \"lat lon\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de80de7-0a4a-4ab0-a0e0-911d4e8fd284",
   "metadata": {},
   "source": [
    "### Loop through mask files and underlying data files; add both to temp file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5922839e-e5e5-4d66-a8ed-81c2195808b0",
   "metadata": {},
   "source": [
    "My process has been to adjust the year in the for loop, move temp.nc to a new name, check the results, and then run for a different year. One could also rename temp.nc above to correspond with the year and then not bother with renaming the files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be517570-9b0c-4f87-9f0b-07de5e9378c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting at 2025-05-08 10:55:26.640462\n",
      "wrote netcdf at 2025-05-08 10:55:26.697333\n"
     ]
    }
   ],
   "source": [
    "if not inference:  # ADD MASKS AND UNDERLYING DATA TO NCFILE FOR TRAINING AND TESTING\n",
    "    print('starting at {}'.format(dt.datetime.now()))\n",
    "    if region == 'antarctic':\n",
    "        directory_of_underlying_data = \"/glade/derecho/scratch/tking/cgnet/high_lat_QC/from_nersc/2dlatlon/sh_polar/renamed/\"\n",
    "    elif region == 'arctic':\n",
    "        directory_of_underlying_data = \"/glade/derecho/scratch/tking/cgnet/high_lat_QC/from_nersc/2dlatlon/nh_polar/renamed/\"\n",
    "    time_index = -1\n",
    "    \n",
    "\n",
    "    # The years below correspond to the mask's listed years (ie, incorrect years from round 1)\n",
    "    # For processing data, I recommend running one year at a time and then renaming temp.nc to match whatever that year is\n",
    "    for year in [year_val]:\n",
    "        # gather data from a particular round of QC\n",
    "        # some of these required different data processing steps which are adjusted based on the round_val below\n",
    "        if region == 'antarctic':\n",
    "            if round_val == 1:\n",
    "                mask_file_list = sorted(glob.glob('/glade/work/tking/cgnet/QA_xml/round_1/h5/qa*/antarctic/netcdfs/data-{}-*'.format(year)))\n",
    "            elif round_val == 2:\n",
    "                mask_file_list = sorted(glob.glob('/glade/work/tking/cgnet/QA_xml/round_2/h5/qa*/antarctic/netcdfs/data-{}-*'.format(year)))\n",
    "            elif round_val == 3:\n",
    "                mask_file_list = sorted(glob.glob('/glade/work/tking/cgnet/QA_xml/round_3/h5/qa*/antarctic/netcdfs/data-{}-*'.format(year)))\n",
    "        if region == 'arctic':\n",
    "            if round_val == 1:\n",
    "                mask_file_list = sorted(glob.glob('/glade/work/tking/cgnet/QA_xml/round_1/h5/qa*/arctic/netcdfs/data-{}-*'.format(year)))\n",
    "            elif round_val == 2:\n",
    "                mask_file_list = sorted(glob.glob('/glade/work/tking/cgnet/QA_xml/round_2/h5/qa*/arctic/netcdfs/data-{}-*'.format(year)))\n",
    "            elif round_val == 3:\n",
    "                mask_file_list = sorted(glob.glob('/glade/work/tking/cgnet/QA_xml/round_3/h5/qa*/arctic/netcdfs/data-{}-*'.format(year)))\n",
    "\n",
    "        if region == 'arctic' and round_val == 2:\n",
    "            round_val = 1  # Treat arctic round 2 as round 1 for years to line up!\n",
    "        \n",
    "        if round_val == 1:\n",
    "            shifted_year = year - 1\n",
    "        else:\n",
    "            shifted_year = year\n",
    "\n",
    "        # Bug fix from 2000 data being pulled in previously:\n",
    "        if round_val == 1:\n",
    "            shifted_year = 2000\n",
    "\n",
    "        # get underlying data\n",
    "        tmq_ds = xr.open_dataset(directory_of_underlying_data+'tmq/{}'.format(tmq_dict[shifted_year]))\n",
    "        psl_ds = xr.open_dataset(directory_of_underlying_data+'psl/{}'.format(psl_dict[shifted_year]))\n",
    "        ivt_ds = xr.open_dataset(directory_of_underlying_data+'ivt/{}'.format(ivt_dict[shifted_year]))\n",
    "        pr_ds = xr.open_dataset(directory_of_underlying_data+'pr/{}'.format(pr_dict[shifted_year]))\n",
    "\n",
    "        # loop through mask files, get corresponding underlying data, and add to temporary file\n",
    "        for mask_file in mask_file_list[:]:\n",
    "            time = mask_file.split('/')[-1].split('data-')[1].split('.nc')[0].split('-00-2')[0].split('_')[0]\n",
    "            time_year = int(time.split('-')[0])  # or = year\n",
    "\n",
    "            # For round 1, assume that the nc file is ~named~ 2001, but underlying data is 2000\n",
    "            if round_val == 1:\n",
    "                time_year = 2001\n",
    "\n",
    "            time_month = int(time.split('-')[1])\n",
    "            time_day = int(time.split('-')[2])\n",
    "            if round_val == 1 or round_val == 2:\n",
    "                time_hour = 22  # all files were 00\n",
    "                time_mins = 30\n",
    "            if round_val == 3:\n",
    "                time_hour = int(time.split('-')[4])*3-2  # 1.5 hours off of time provided... so subtract 2 hr and add a half hour\n",
    "                time_mins = 30\n",
    "                if time_year == 2000:\n",
    "                    if time_month >= 3:  # Account for leap day in march and later months for year 2000\n",
    "                        # if time_day < 31 or 30:\n",
    "                        time_day = time_day+1\n",
    "                        # if time_day==31 or 30:\n",
    "                        #     time_day = 1\n",
    "                        #     time_month = time_month +1\n",
    "                        # we would need a slightly more robust way of doing this if there were masks incorrectly labelled from the last day of the month,\n",
    "                        #     but this works for now\n",
    "            try:\n",
    "                date_number = date2num(dt.datetime(time_year, time_month, time_day, time_hour, time_mins), 'hours since 1970-01-01')\n",
    "            except:\n",
    "                if time_month == 6 and time_day ==31:\n",
    "                    continue\n",
    "            # ---------------------------------------------------------------------------------\n",
    "            # Fix for indexing bugs is now in all_chey_arctic.ipynb and all_chey_antarctic.ipynb,\n",
    "            # So this fix will not be needed after round 1 data is processed.\n",
    "            # In this fix, if data is from 2000, adjust days by 4, otherwise, adjust days by 5 (to account for leap days)\n",
    "            if round_val == 1:\n",
    "                if time_year == 2000:\n",
    "                    leap_year_adjustment = 4\n",
    "                if time_year >= 2001:\n",
    "                    leap_year_adjustment = 5\n",
    "                if time_month in [1, 3, 5, 7, 8, 10, 12]:  # months with 31 days\n",
    "                    days_in_month = 31\n",
    "                    # adjust year for indexing issue unless last few days in file (because of leap year, these got included in correct file)\n",
    "                    if time_month == 12 and time_day < (days_in_month - leap_year_adjustment):\n",
    "                        time_year = time_year - 1\n",
    "                    else:\n",
    "                        time_year = time_year - 1\n",
    "                    if time_day < (days_in_month - leap_year_adjustment):\n",
    "                        time_day = time_day + leap_year_adjustment\n",
    "                    else:\n",
    "                        time_day = ((time_day + leap_year_adjustment) - days_in_month) + 1  # add 4 days for leap year, subtract days in month, add 1 because month starts at 1 not 0.\n",
    "                        time_month += 1  # use one of the first few days in the next month\n",
    "                        if time_month == 13:  # no 13th month, so loop back to January\n",
    "                            time_month = 1\n",
    "                elif time_month in [4, 6, 9, 11]:  # months with 30 days\n",
    "                    time_year = time_year - 1\n",
    "                    days_in_month = 30\n",
    "                    if time_day < (days_in_month - leap_year_adjustment):\n",
    "                        time_day = time_day + leap_year_adjustment\n",
    "                    else:\n",
    "                        time_day = ((time_day + leap_year_adjustment) - days_in_month) + 1\n",
    "                        time_month += 1  # use one of the first few days in the next month\n",
    "                elif time_month == 2:\n",
    "                    time_year = time_year - 1\n",
    "                    days_in_month = 28\n",
    "                    if time_day < (days_in_month - leap_year_adjustment):\n",
    "                        time_day = time_day + leap_year_adjustment\n",
    "                    else:\n",
    "                        time_day = ((time_day + leap_year_adjustment) - days_in_month) + 1\n",
    "                        time_month += 1  # use one of the first few days in the next month\n",
    "            # ---------------------------------------------------------------------------------\n",
    "\n",
    "            # format strings for use in netcdf date attribute\n",
    "            if time_month < 10:\n",
    "                time_m_formatted = '0'+str(time_month)\n",
    "            else:\n",
    "                time_m_formatted = str(time_month)\n",
    "            if time_day < 10:\n",
    "                time_d_formatted = '0'+str(time_day)\n",
    "            else:\n",
    "                time_d_formatted = str(time_day)\n",
    "\n",
    "            # Some files (mostly 2002) start at _0 and sometimes just end with -2 and if repeat date, include _1.nc\n",
    "            # only increase time index if moving to a new time; otherwise additional masks can be in a new sample_id of the same time index\n",
    "            if mask_file[-5:-4] == '_':\n",
    "                sample_id = int(mask_file[-4:-3])\n",
    "            else:\n",
    "                sample_id = 0\n",
    "\n",
    "            if sample_id == 0:\n",
    "                time_index += 1\n",
    "\n",
    "            qa_aa_ds = xr.open_dataset(mask_file)\n",
    "\n",
    "            # reorientation needed for viewing with matplotlib\n",
    "            qa_aa_ds.ar_masks['phony_dim_0'] = qa_aa_ds.ar_masks['phony_dim_0'][::-1]\n",
    "            qa_aa_ds.reindex(phony_dim_0=list(reversed(qa_aa_ds.phony_dim_0)))\n",
    "\n",
    "            # fill in date and datesec\n",
    "            date[time_index] = str(time_year)+time_m_formatted+time_d_formatted\n",
    "            if round_val == 1 or round_val == 2:\n",
    "                datesec[time_index] = '81000'  # corresponds to 22:30\n",
    "            if round_val == 3:\n",
    "                datesec[time_index] = str((3600*time_hour)+(60*time_mins))\n",
    "            # fill in ar_mask\n",
    "            ar_mask[time_index, sample_id, :, :] = qa_aa_ds.ar_masks\n",
    "            # get data arrays for underlying data\n",
    "            pr = xr.DataArray(pr)\n",
    "            psl = xr.DataArray(psl)\n",
    "            tmq = xr.DataArray(tmq)\n",
    "            ivt = xr.DataArray(ivt)\n",
    "\n",
    "            # fill in underlying data parts of temporary ncfile\n",
    "            if region == 'antarctic':\n",
    "                if round_val == 1 or round_val == 2:\n",
    "                    ncfile['pr'][time_index, :, :] = pr_ds.sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True), method='nearest').isel(height=0).to_array().dropna('variable').dropna('nvertices').dropna('nb2')[5,:,:,1,1]\n",
    "                    ncfile['psl'][time_index, :, :] = psl_ds.sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True), method='nearest').to_array().dropna('variable').dropna('nvertices').dropna('nbnd')[5,:,:,1,1]\n",
    "                    ncfile['tmq'][time_index, :, :] = tmq_ds.sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True), method='nearest').to_array().dropna('variable').dropna('nvertices').dropna('nbnd')[5,:,:,1,1]\n",
    "                    ncfile['ivt'][time_index, :, :] = ivt_ds.sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True), method='nearest').to_array().dropna('variable').dropna('nvertices').dropna('bound')[7,:,:,1,1]\n",
    "                if round_val == 3:\n",
    "                    ncfile['pr'][time_index, :, :] = pr_ds.sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True), method='nearest').isel(height=0).to_array().dropna('variable').dropna('nvertices').dropna('nb2')[3,:,:,1,1]\n",
    "                    ncfile['psl'][time_index, :, :] = psl_ds.sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True), method='nearest').to_array().dropna('variable').dropna('nvertices').dropna('nbnd')[3,:,:,1,1]\n",
    "                    ncfile['tmq'][time_index, :, :] = tmq_ds.sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True), method='nearest').to_array().dropna('variable').dropna('nvertices').dropna('nbnd')[3,:,:,1,1]\n",
    "                    ncfile['ivt'][time_index, :, :] = ivt_ds.sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True), method='nearest').to_array().dropna('variable').dropna('nvertices').dropna('bound')[6,:,:,1,1]\n",
    "            if region == 'arctic':\n",
    "                try:\n",
    "                    ncfile['pr'][time_index, :, :] = pr_ds['pr'].sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True)).values\n",
    "                except:\n",
    "                    _, index = np.unique(pr_ds['time'], return_index=True)\n",
    "                    pr_ds = pr_ds.isel(time=index)\n",
    "                    ncfile['pr'][time_index, :, :] = pr_ds['pr'].sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True),method='nearest').values\n",
    "                try:\n",
    "                    ncfile['psl'][time_index, :, :] = psl_ds['psl'].sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True)).values\n",
    "                except:\n",
    "                    _, index = np.unique(psl_ds['time'], return_index=True)\n",
    "                    psl_ds = psl_ds.isel(time=index)\n",
    "                    ncfile['psl'][time_index, :, :] = psl_ds['psl'].sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True), method='nearest').values\n",
    "                try:\n",
    "                    ncfile['tmq'][time_index, :, :] = tmq_ds['tmq'].sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True)).values\n",
    "                except:\n",
    "                    _, index = np.unique(tmq_ds['time'], return_index=True)\n",
    "                    tmq_ds = tmq_ds.isel(time=index)\n",
    "                    ncfile['tmq'][time_index, :, :] = tmq_ds['tmq'].sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True), method='nearest').values\n",
    "                try:\n",
    "                    ncfile['ivt'][time_index, :, :] = ivt_ds['ivt'].sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True)).values    \n",
    "                except:\n",
    "                    _, index = np.unique(ivt_ds['time'], return_index=True)\n",
    "                    ivt_ds = ivt_ds.isel(time=index)\n",
    "                    ncfile['ivt'][time_index, :, :] = ivt_ds['ivt'].sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True), method='nearest' ).values                            \n",
    "            print('all data added for {}'.format(str(time_year)+' '+time_m_formatted+' '+time_d_formatted))\n",
    "\n",
    "            # time reported on netcdf frame:\n",
    "            time_val = cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True) - cftime.DatetimeNoLeap(1970, 1, 1, 0, 0, 0, 0, has_year_zero=True)\n",
    "            ncfile['time'][time_index] = (time_val.days * 24) + (time_val.seconds / 3600)\n",
    "\n",
    "            # print(time_m_formatted)\n",
    "            # if time_m_formatted == '04':  # TODO: BREAK FOR TESTING PURPOSES on 4th layer- REMOVE ME WHEN ACTUALLY RUNNING!!!\n",
    "            #     # ncfile.close()\n",
    "            #     break\n",
    "\n",
    "        # write netcdf\n",
    "        ncfile.close()\n",
    "    print('wrote netcdf at {}'.format(dt.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8338e276-0bec-4205-abfe-647ecb09ba5b",
   "metadata": {},
   "source": [
    "Once these files are all created, run split_script with batch_split.sh. This creates one file per timestep.\n",
    "\n",
    "Then, there's a bunch of nco commands shown below to run on every file (where `$i` is the filename) that has been split:\n",
    "\n",
    "```\n",
    "foreach i (*.nc)\n",
    "    echo $i\n",
    "    Command(s) below (do in chunks as shown below, either one or a few commands at a time)\n",
    "    end\n",
    "\n",
    "ncrename -O -v ar_mask,LABELS $i\n",
    "ncatted -O -a coordinates,ivt,d,, $i\n",
    "ncatted -O -a coordinates,tmq,d,, $i\n",
    "ncatted -O -a coordinates,pr,d,, $i\n",
    "ncatted -O -a coordinates,psl,d,, $i\n",
    "\n",
    "ncap2 -s “LABELS=int(LABELS)” $i labels/$i\n",
    "    * Make sure use the right quotes\n",
    "\n",
    "cd labels\n",
    "\n",
    "ncrename -v lon,longitude -v lat,latitude $i\n",
    "\n",
    "ncrename -v x,lon -v y,lat $i\n",
    "ncrename -d x,lon -d y,lat $i\n",
    "\n",
    "ncatted -O -a flag_meanings,LABELS,d,, $i\n",
    "ncatted -O -a flag_values,LABELS,d,, $i\n",
    "ncatted -O -a long_name,LABELS,d,, $i\n",
    "ncatted -O -a standard_name,LABELS,d,, $i\n",
    "ncatted -a description,LABELS,c,c,“0: Background, 1: Atmospheric_River” $i\n",
    "    * Make sure use the right quotes\n",
    "\n",
    "ncks -d sample_id,0 $i sample_0/$i\n",
    "    Do through sample_5\n",
    "    Determine which files are all zeroes & remove (these masks were not written)\n",
    "        Eg, for sample_id 1: ls ../../../../../../round_3/h5/qa1/antarctic/*_1.h5\n",
    "        Ls ../../../../../../round_3/h5/qa1/antarctic/*-?.h5\n",
    "        Find which dates correspond with which file number\n",
    "        Move those to a keep dir\n",
    "        Remove everything else and then remove keep dir\n",
    "ncap2 -s tmq=float(tmq) -s latitude=float(latitude) -s longitude=float(longitude) -s pr=float(pr) -s psl=float(psl) -s ivt=float(ivt) -s datesec=float(datesec) -s time=float(time) -s date=float(date) $i  ncap/$i # add quotes!\n",
    "ncwa -O -a sample_id $i $i\n",
    "\n",
    "ncpdq -M dbl_flt \"$f\" \"../$f\"\n",
    "\n",
    "```\n",
    "\n",
    "And finally separate randomly into test/train directories (20/80)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578f613d-d7f5-4125-bfb3-3f15b253c09f",
   "metadata": {},
   "source": [
    "## make INFERENCE dataset (unlabeled; no masks included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4178e0c9-fac9-49a8-b017-9dbc0a00d4d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting at 2025-05-07 16:58:13.060281\n",
      "['2000', '01', '01', '10']\n",
      "all data added for 20000101\n",
      "['2000', '01', '01', '20']\n",
      "all data added for 20000101\n",
      "['2000', '01', '01', '30']\n",
      "all data added for 20000101\n",
      "['2000', '01', '01', '40']\n",
      "all data added for 20000101\n",
      "['2000', '01', '01', '50']\n",
      "all data added for 20000101\n",
      "['2000', '01', '01', '60']\n",
      "all data added for 20000101\n",
      "['2000', '01', '01', '70']\n",
      "all data added for 20000101\n",
      "['2000', '01', '01', '80']\n",
      "all data added for 20000101\n",
      "['2000', '01', '02', '10']\n",
      "all data added for 20000102\n",
      "['2000', '01', '02', '20']\n",
      "all data added for 20000102\n",
      "['2000', '01', '02', '30']\n",
      "all data added for 20000102\n",
      "['2000', '01', '02', '40']\n",
      "all data added for 20000102\n",
      "['2000', '01', '02', '50']\n",
      "all data added for 20000102\n",
      "['2000', '01', '02', '60']\n",
      "all data added for 20000102\n",
      "['2000', '01', '02', '70']\n",
      "all data added for 20000102\n",
      "['2000', '01', '02', '80']\n",
      "all data added for 20000102\n",
      "['2000', '01', '03', '10']\n",
      "all data added for 20000103\n",
      "['2000', '01', '03', '20']\n",
      "all data added for 20000103\n",
      "['2000', '01', '03', '30']\n",
      "all data added for 20000103\n",
      "['2000', '01', '03', '40']\n",
      "all data added for 20000103\n",
      "['2000', '01', '03', '50']\n",
      "all data added for 20000103\n",
      "['2000', '01', '03', '60']\n",
      "all data added for 20000103\n",
      "['2000', '01', '03', '70']\n",
      "all data added for 20000103\n",
      "['2000', '01', '03', '80']\n",
      "all data added for 20000103\n",
      "['2000', '01', '04', '10']\n",
      "all data added for 20000104\n",
      "['2000', '01', '04', '20']\n",
      "all data added for 20000104\n",
      "['2000', '01', '04', '30']\n",
      "all data added for 20000104\n",
      "['2000', '01', '04', '40']\n",
      "all data added for 20000104\n",
      "['2000', '01', '04', '50']\n",
      "all data added for 20000104\n",
      "['2000', '01', '04', '60']\n",
      "all data added for 20000104\n",
      "['2000', '01', '04', '70']\n",
      "all data added for 20000104\n",
      "['2000', '01', '04', '80']\n",
      "all data added for 20000104\n",
      "['2000', '01', '05', '10']\n",
      "all data added for 20000105\n",
      "['2000', '01', '05', '20']\n",
      "all data added for 20000105\n",
      "['2000', '01', '05', '30']\n",
      "all data added for 20000105\n",
      "['2000', '01', '05', '40']\n",
      "all data added for 20000105\n",
      "['2000', '01', '05', '50']\n",
      "all data added for 20000105\n",
      "['2000', '01', '05', '60']\n",
      "all data added for 20000105\n",
      "['2000', '01', '05', '70']\n",
      "all data added for 20000105\n",
      "['2000', '01', '05', '80']\n",
      "all data added for 20000105\n",
      "['2000', '01', '06', '10']\n",
      "all data added for 20000106\n",
      "['2000', '01', '06', '20']\n",
      "all data added for 20000106\n",
      "['2000', '01', '06', '30']\n",
      "all data added for 20000106\n",
      "['2000', '01', '06', '40']\n",
      "all data added for 20000106\n",
      "['2000', '01', '06', '50']\n",
      "all data added for 20000106\n",
      "['2000', '01', '06', '60']\n",
      "all data added for 20000106\n",
      "['2000', '01', '06', '70']\n",
      "all data added for 20000106\n",
      "['2000', '01', '06', '80']\n",
      "all data added for 20000106\n",
      "['2000', '01', '07', '10']\n",
      "all data added for 20000107\n",
      "['2000', '01', '07', '20']\n",
      "all data added for 20000107\n",
      "['2000', '01', '07', '30']\n",
      "all data added for 20000107\n",
      "['2000', '01', '07', '40']\n",
      "all data added for 20000107\n",
      "['2000', '01', '07', '50']\n",
      "all data added for 20000107\n",
      "['2000', '01', '07', '60']\n",
      "all data added for 20000107\n",
      "['2000', '01', '07', '70']\n",
      "all data added for 20000107\n",
      "['2000', '01', '07', '80']\n",
      "all data added for 20000107\n",
      "['2000', '01', '08', '10']\n",
      "all data added for 20000108\n",
      "['2000', '01', '08', '20']\n",
      "all data added for 20000108\n",
      "['2000', '01', '08', '30']\n",
      "all data added for 20000108\n",
      "['2000', '01', '08', '40']\n",
      "all data added for 20000108\n",
      "['2000', '01', '08', '50']\n",
      "all data added for 20000108\n",
      "['2000', '01', '08', '60']\n",
      "all data added for 20000108\n",
      "['2000', '01', '08', '70']\n",
      "all data added for 20000108\n",
      "['2000', '01', '08', '80']\n",
      "all data added for 20000108\n",
      "['2000', '01', '09', '10']\n",
      "all data added for 20000109\n",
      "['2000', '01', '09', '20']\n",
      "all data added for 20000109\n",
      "['2000', '01', '09', '30']\n",
      "all data added for 20000109\n",
      "['2000', '01', '09', '40']\n",
      "all data added for 20000109\n",
      "['2000', '01', '09', '50']\n",
      "all data added for 20000109\n",
      "['2000', '01', '09', '60']\n",
      "all data added for 20000109\n",
      "['2000', '01', '09', '70']\n",
      "all data added for 20000109\n",
      "['2000', '01', '09', '80']\n"
     ]
    }
   ],
   "source": [
    "if inference:\n",
    "    print('starting at {}'.format(dt.datetime.now()))\n",
    "    directory_of_underlying_data = \"/glade/derecho/scratch/tking/cgnet/high_lat_QC/from_nersc/2dlatlon/sh_polar/renamed/\"\n",
    "    time_index = -1\n",
    "    round_val = 3 # change this value to indicate whether or not bug fix from round 1 is used\n",
    "\n",
    "    # The years below correspond to the mask's listed years (ie, incorrect years from round 1)\n",
    "    # For processing data, I recommend running one year at a time and then renaming temp.nc to match whatever that year is\n",
    "    for year in [2000]:\n",
    "        if round_val == 3:\n",
    "            mask_file_list = sorted(glob.glob('/glade/work/tking/cgnet/QA_xml/round_3/h5/qa*/antarctic/netcdfs/data-{}-*'.format(year)))\n",
    "            train_or_test_dates = []\n",
    "            for mask in mask_file_list:\n",
    "                train_or_test_dates.append(mask.split('/')[-1].split('data-')[1].split('.nc')[0].split('-00-2')[0].split('_')[0])\n",
    "            shifted_year = year\n",
    "\n",
    "        # determine list of all possible dates and then remove dates that were previously used\n",
    "        possible_dates = []\n",
    "        for month in range(1, 13):\n",
    "            if month < 10:\n",
    "                month = '0'+str(month)\n",
    "            else:\n",
    "                month = str(month)\n",
    "            for day in range(1, 29):\n",
    "                if day < 10:\n",
    "                    day = '0'+str(day)\n",
    "                else:\n",
    "                    day = str(day)\n",
    "                for hour in range(1, 9):\n",
    "                    hour = str(hour) + '0'\n",
    "                    possible_dates.append(f'data-2000-{month}-{day}-{hour}')\n",
    "        not_in_labeled_dates = []\n",
    "        for specific_time in possible_dates:\n",
    "            if possible_dates not in train_or_test_dates:\n",
    "                not_in_labeled_dates.append(specific_time)\n",
    "\n",
    "        # get underlying data\n",
    "        tmq_ds = xr.open_dataset(directory_of_underlying_data+'tmq/{}'.format(tmq_dict[shifted_year]))\n",
    "        psl_ds = xr.open_dataset(directory_of_underlying_data+'psl/{}'.format(psl_dict[shifted_year]))\n",
    "        ivt_ds = xr.open_dataset(directory_of_underlying_data+'ivt/{}'.format(ivt_dict[shifted_year]))\n",
    "        pr_ds = xr.open_dataset(directory_of_underlying_data+'pr/{}'.format(pr_dict[shifted_year]))\n",
    "\n",
    "        # loop through times, get corresponding underlying data, and add to temporary file\n",
    "        for time in not_in_labeled_dates:\n",
    "            time = time.split('-')[1:]\n",
    "            print(time)\n",
    "            time_year = int(time[0])\n",
    "\n",
    "            time_month = int(time[1])\n",
    "            time_day = int(time[2])\n",
    "            if round_val==3:\n",
    "                time_hour = int(time[3][0])*3-2  # 1.5 hours off of time provided... so subtract 2 hr and add a half hour\n",
    "                time_mins = 30\n",
    "                if time_year == 2000:\n",
    "                    if time_month >= 3:  # Account for leap day in march and later months for year 2000\n",
    "                        # if time_day < 31 or 30:\n",
    "                        time_day = time_day+1\n",
    "\n",
    "            date_number = date2num(dt.datetime(time_year, time_month, time_day, time_hour, time_mins), 'hours since 1970-01-01')\n",
    "\n",
    "            # ---------------------------------------------------------------------------------\n",
    "\n",
    "            # format strings for use in netcdf date attribute\n",
    "            if time_month < 10:\n",
    "                time_m_formatted = '0'+str(time_month)\n",
    "            else:\n",
    "                time_m_formatted = str(time_month)\n",
    "            if time_day < 10:\n",
    "                time_d_formatted = '0'+str(time_day)\n",
    "            else:\n",
    "                time_d_formatted = str(time_day)\n",
    "\n",
    "            sample_id = 0\n",
    "\n",
    "            if sample_id == 0:\n",
    "                time_index += 1\n",
    "\n",
    "            # fill in date and datesec\n",
    "            date[time_index] = str(time_year)+time_m_formatted+time_d_formatted\n",
    "            if round_val == 1 or round_val == 2:\n",
    "                datesec[time_index] = '81000'  # corresponds to 22:30\n",
    "            if round_val == 3:\n",
    "                datesec[time_index] = str((3600*time_hour)+(60*time_mins))\n",
    "            # get data arrays for underlying data\n",
    "            pr = xr.DataArray(pr)\n",
    "            psl = xr.DataArray(psl)\n",
    "            tmq = xr.DataArray(tmq)\n",
    "            ivt = xr.DataArray(ivt)\n",
    "\n",
    "            # fill in underlying data parts of temporary ncfile\n",
    "            if round_val == 1 or round_val == 2:\n",
    "                ncfile['pr'][time_index, :, :] = pr_ds.sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True), method='nearest').isel(height=0).to_array().dropna('variable').dropna('nvertices').dropna('nb2')[5,:,:,1,1]\n",
    "                ncfile['psl'][time_index, :, :] = psl_ds.sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True), method='nearest').to_array().dropna('variable').dropna('nvertices').dropna('nbnd')[5,:,:,1,1]\n",
    "                ncfile['tmq'][time_index, :, :] = tmq_ds.sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True), method='nearest').to_array().dropna('variable').dropna('nvertices').dropna('nbnd')[5,:,:,1,1]\n",
    "                ncfile['ivt'][time_index, :, :] = ivt_ds.sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True), method='nearest').to_array().dropna('variable').dropna('nvertices').dropna('bound')[7,:,:,1,1]\n",
    "            if round_val==3:\n",
    "                ncfile['pr'][time_index, :, :] = pr_ds.sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True), method='nearest').isel(height=0).to_array().dropna('variable').dropna('nvertices').dropna('nb2')[3,:,:,1,1]\n",
    "                ncfile['psl'][time_index, :, :] = psl_ds.sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True), method='nearest').to_array().dropna('variable').dropna('nvertices').dropna('nbnd')[3,:,:,1,1]\n",
    "                ncfile['tmq'][time_index, :, :] = tmq_ds.sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True), method='nearest').to_array().dropna('variable').dropna('nvertices').dropna('nbnd')[3,:,:,1,1]\n",
    "                ncfile['ivt'][time_index, :, :] = ivt_ds.sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True), method='nearest').to_array().dropna('variable').dropna('nvertices').dropna('bound')[6,:,:,1,1]\n",
    "\n",
    "            print('all data added for {}'.format(str(time_year)+time_m_formatted+time_d_formatted))\n",
    "\n",
    "            # time reported on netcdf frame:\n",
    "            time_val = cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True) - cftime.DatetimeNoLeap(1970, 1, 1, 0, 0, 0, 0, has_year_zero=True)\n",
    "            ncfile['time'][time_index] = (time_val.days * 24) + (time_val.seconds / 3600)\n",
    "        # write netcdf\n",
    "        ncfile.close()\n",
    "    print('wrote netcdf at {}'.format(dt.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba076e3-e370-4c82-bb8a-16c51ebec4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PolarARs",
   "language": "python",
   "name": "polarars"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
