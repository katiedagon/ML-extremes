{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41157662-0d9e-4b44-a9d2-38fb15f98dda",
   "metadata": {},
   "source": [
    "# Convert Lat/Lon Files to Polar Stereographic Files\n",
    "\n",
    "This notebook provides a workflow for converting lat/lon netcdf files to polar stereographic coordinates. Then, we combine the polar stereographic underlying data files and mask files in order to use as input to cgnet. To get the data into the proper format, we needed to do the following:\n",
    " - utilize a template file as a base for modifying netcdf files of the existing mask arrays. The template file includes all proper GIS attributes.\n",
    " - python script reads the Geotiff file and creates a netcdf file with the x,y,lat,lon coordinate dimensions and variables\n",
    "\n",
    "Authors:\n",
    " - John Truesdale\n",
    " - Teagan King"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1271ca6c-ebe4-4ac6-b348-67429dab4c41",
   "metadata": {},
   "source": [
    "Some background notes from John on methods for regridding:\n",
    "----------------------------------------------------------\n",
    " - used QGIS to assign a standard stereographic coordinate and create a GeoTiff file version of one of the Polar jpg plots\n",
    " - Giving a standard location to each jpeg pixel basically consisted of finding similar features at the pixel level (the tip of an island, the most inset part of a promenent bay) between our polar jpegs and a map that is already georeferenced and has known locations for those pixels. If you choose 3-10 pixels in common you can create a linear regression that will map out all the rest of the pixels on your jpeg.\n",
    " - Once the GIS application was able to calculate the transform to go from pixel to a standard coordinate system, I saved all that information in a GeoTiff file.\n",
    " - The polar projection jpegs on which the climatenet masks are drawn were created from python matplotlib and you can grab the coordinate information from matplotlib; this was checked with QGIS\n",
    " - Because the LLNL polar jpegs are a projected coordinate, the underlying unit in a stereographic projection is meters.  The x and y variables on the GeoTiff and converted netcdf file contain meter offsets of every pixel (row,col) of the ar_mask array with respect to one of the standard south pole stereographic coordinate systems.\n",
    " -   When you look at the square projected polar image you see that the longitude lines converge at the pole and latitudes are a set of nested circles.  When you are describing this grid in lat/lon coordinates it is known as a curvilinear grid where each pixel (array location) requires a unique lat/lon pair to specify its position on a regular grid. A straight line along any row or column of the jpeg raster (or ar_mask array) will intersect different lat lon values for every pixel.  The coordinate information for our rectangular ar_mask array therefore contains lat and lon variables that are two dimensions and describe the entire grid of 1152x1152 points with unique lat/lon values for each pixel (array location) of ar_mask. The standard netcdf way of denoting a curvilinear coordinate is by creating the dimensions that define the size of the ar_mask array (x,y), adding lat/lon variables that are each dimensioned (x,y) containing the lat/lon coordinates of that point, and finally adding metadata to the ar_mask array noting that the coordinates for this variable are not the dimension variables x,y but the lat/lon variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df8a372-dbb9-4d9e-804b-d74cc64c3e57",
   "metadata": {},
   "source": [
    "Generate remapped IVT/TMQ/etc underlying data:  \n",
    " - ESMF_RegridWeightGen --ignore_unmapped --dst_regional -m bilinear -w map_fv0.23x0.31_to_sp_stereo_near.nc -s /glade/p/cesmdata/inputdata/share/scripgrids/fv0.23x0.31_141008.nc -d /glade/u/home/jet/sp_stereographic_SCRIP.nc  \n",
    " - ncremap -m ./map_fv0.23x0.31_to_sp_stereo_near.nc -i windhusavi_3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200001-200012.nc -o polar_ivt/windhusavi_3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200001-200012.nc  \n",
    " - see /glade/scratch/tking/cgnet/high_lat_QC/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b27c606-38cb-492c-a4d8-f9ef05b2d915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from rasterio.warp import transform\n",
    "import urllib.request\n",
    "import xarray as xr\n",
    "import glob\n",
    "from netCDF4 import date2num\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import cftime\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321daf7f-ce7f-4fba-a2d4-56ac4ab905da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Generate template file (only needs to be done once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d231450c-635f-4340-8e10-51e781b9baf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Read the data\n",
    "# input_path = '/glade/work/tking/cgnet/polar_regridding/data-2003-04-29-02-0-copy-sav1.tif'\n",
    "\n",
    "# da = xr.open_rasterio(input_path)\n",
    "# yval=da['y']\n",
    "# ryval=np.flip(yval)\n",
    "# # Compute the lon/lat coordinates with rasterio.warp.transform\n",
    "# ny, nx = len(da['y']), len(da['x'])\n",
    "# x, y = np.meshgrid(da['x'], ryval)\n",
    "# # Rasterio works with 1D arrays\n",
    "# lon, lat = transform(da.crs, {'init': 'EPSG:4326'},\n",
    "#                      x.flatten(), y.flatten())\n",
    "# lon = np.asarray(lon).reshape((ny, nx))\n",
    "# lat = np.asarray(lat).reshape((ny, nx))\n",
    "# da.coords['lon'] = (('y', 'x'), lon)\n",
    "# da.coords['lat'] = (('y', 'x'), lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3090f32-4e0e-43ae-8b4c-8a30adf17e6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# da.to_netcdf(path='/glade/work/tking/cgnet/polar_regridding/data-2003-04-29-02-0-sav1-rev-latlon.nc')\n",
    "# # use for just antarctic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d343dcca-82a8-4400-b39b-536cc612c5a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## set up dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f031f803-5c94-4e14-aff7-43be24698ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dask\n",
    "import dask\n",
    "\n",
    "# Use dask jobqueue\n",
    "from dask_jobqueue import PBSCluster\n",
    "\n",
    "# Import a client\n",
    "from dask.distributed import Client\n",
    "\n",
    "# Setup your PBSCluster\n",
    "nmem='25GB' # specify memory here so it duplicates below\n",
    "cluster = PBSCluster(\n",
    "    cores=1, # The number of cores you want\n",
    "    memory=nmem, # Amount of memory\n",
    "    processes=1, # How many processes\n",
    "    queue='casper', # The type of queue to utilize (/glade/u/apps/dav/opt/usr/bin/execcasper)\n",
    "    local_directory='/glade/scratch/$USER/local_dask', # Use your local directory\n",
    "    resource_spec='select=1:ncpus=1:mem='+nmem, # Specify resources\n",
    "    account='P93300313', # Input your project ID here, previously this was known as 'project', now is 'account'\n",
    "    walltime='08:00:00', # Amount of wall time\n",
    "    # interface='ib0', # Interface to use\n",
    ")\n",
    "\n",
    "# Scale up\n",
    "cluster.scale(10)\n",
    "\n",
    "# Change your url to the dask dashboard so you can see it\n",
    "dask.config.set({'distributed.dashboard.link':'https://jupyterhub.hpc.ucar.edu/stable/user/{USER}/proxy/{port}/status'})\n",
    "\n",
    "# Setup your client\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69bfb96-a4d8-45b2-a457-732854f3824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e59a9d-b213-4494-af8b-2fec61683c25",
   "metadata": {
    "tags": []
   },
   "source": [
    "## define dictionary of file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d91b073-f069-481d-8f67-582234153ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmq_dict = {2000: \"prw_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200001010000-200012312100.nc\",\n",
    "            2001: \"prw_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200101010000-200112312100.nc\",\n",
    "            2002: \"prw_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200201010000-200212312100.nc\",\n",
    "            2003: \"prw_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200301010000-200312312100.nc\",\n",
    "            2004: \"prw_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200401010000-200412312100.nc\"}\n",
    "\n",
    "ivt_dict = {2000: \"windhusavi_3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200001-200012.nc\",\n",
    "            2001: \"windhusavi_3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200101-200112.nc\",\n",
    "            2002: \"windhusavi_3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200201-200212.nc\",\n",
    "            2003: \"windhusavi_3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200301-200312.nc\",\n",
    "            2004: \"windhusavi_3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200401-200412.nc\"}\n",
    "\n",
    "psl_dict = {2000: \"psl_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200001010000-200012312100.nc\",\n",
    "            2001: \"psl_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200101010000-200112312100.nc\",\n",
    "            2002: \"psl_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200201010000-200212312100.nc\",\n",
    "            2003: \"psl_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200301010000-200312312100.nc\",\n",
    "            2004: \"psl_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200401010000-200412312100.nc\"}\n",
    "\n",
    "pr_dict = {2000: \"pr_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200001010000-200012312359.nc\",\n",
    "           2001: \"pr_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200101010000-200112312359.nc\",\n",
    "           2002: \"pr_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200201010000-200212312359.nc\",\n",
    "           2003: \"pr_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200301010000-200312312359.nc\",\n",
    "           2004: \"pr_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200401010000-200412312359.nc\",\n",
    "           2005: \"pr_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200501010000-200512052359.nc\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c2fbc3-0d3e-4892-8b7a-81bb1234bb61",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Change to 2-dimensional lat/lon before regridding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ec8a12-cb01-4aaf-bd2a-870702cf6141",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Regrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93084bb4-aee5-447e-9a68-8cc4f9d559e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# only do once\n",
    "\n",
    "for var in ['ivt']: # pr , 'psl', 'tmq'\n",
    "    if var=='pr':\n",
    "        dictionary = pr_dict\n",
    "    elif var=='psl':\n",
    "        dictionary = psl_dict\n",
    "    elif var=='ivt':\n",
    "        dictionary = ivt_dict\n",
    "    elif var=='tmq':\n",
    "        dictionary = tmq_dict\n",
    "    for year in [2000]: #2001,2002,2003,2004]:\n",
    "        ds_before_regrid = xr.open_dataset('/glade/derecho/scratch/tking/cgnet/high_lat_QC/from_nersc/2dlatlon/{}'.format(dictionary[year]))\n",
    "        ds_before_regrid\n",
    "        # we want this to be two dimensional lat/lon instead of 1\n",
    "\n",
    "        # mesh is one useful tool but could do by hand\n",
    "        # duplicate lat/lon array for lon/lat number of times\n",
    "        # in order to have lat (y,x) and lon (y,x)\n",
    "\n",
    "        # y is lat\n",
    "        # x is lon\n",
    "        # dimensions should be time, x, y, \n",
    "        # follow example here: https://xesmf.readthedocs.io/en/latest/notebooks/Curvilinear_grid.html\n",
    "        y_len = ds_before_regrid.lon.shape[0]\n",
    "        x_len = ds_before_regrid.lat.shape[0]\n",
    "\n",
    "        ds_before_regrid['lat_val'] = (('y','x'), np.tile(ds_before_regrid.lat, (y_len,1)))\n",
    "        ds_before_regrid['lon_val'] = (('y','x'), np.transpose(np.tile(ds_before_regrid.lon, (x_len,1))))\n",
    "\n",
    "        if var = 'ivt':\n",
    "            ds_before_regrid[var] = ds_before_regrid.windhusavi.swap_dims({'lat':'x','lon':'y'})\n",
    "        elif var = 'psl':\n",
    "            ds_before_regrid[var] = ds_before_regrid.psl.swap_dims({'lat':'x','lon':'y'})\n",
    "        elif var = 'tmq':\n",
    "            ds_before_regrid[var] = ds_before_regrid.prw.swap_dims({'lat':'x','lon':'y'})\n",
    "        elif var = 'pr':\n",
    "            ds_before_regrid[var] = ds_before_regrid.pr.swap_dims({'lat':'x','lon':'y'})\n",
    "\n",
    "        ds_before_regrid = ds_before_regrid.drop_dims('lat')\n",
    "        ds_before_regrid = ds_before_regrid.drop_dims('lon')\n",
    "        \n",
    "        ds_before_regrid.to_netcdf('/glade/derecho/scratch/tking/cgnet/high_lat_QC/from_nersc/updated_latlon/{}'.format(dictionary[year]))\n",
    "        print(\"done with {} year\".format(var), str(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ddf3c6-b2f5-4878-8647-31afac57cb76",
   "metadata": {
    "tags": []
   },
   "source": [
    "## regrid TMQ/IVT/PSL/PR data from lat/lon to polar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77ff4b0-02f7-4124-a4ff-9d1070363738",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submit scripts below in batch scripts, see example at\n",
    "#     /glade/scratch/tking/cgnet/high_lat_QC/from_nersc/2dlatlon/remap_script and batch_remap.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a689f456-8929-46e8-97b2-0a63de555a0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# module load gnu/9.1.0\n",
    "# module load esmf_libs/8.0.0\n",
    "# module load esmf-8.0.0-ncdfio-mpi-O\n",
    "# module load nco/4.7.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bcb116-68b8-4e20-b368-579372a4ceca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set srcgrid=f09\n",
    "# set dstgrid=sp_stereo\n",
    "# set srcgridfile=/glade/p/cesmdata/inputdata/share/scripgrids/fv0.23x0.31_141008.nc\n",
    "# set dstgridfile=/glade/u/home/jet/sp_stereographic_SCRIP.nc\n",
    "# set srcinitfile=prw_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200001010000-200012312100.nc\n",
    "# set dstinitfile=polar_tmq/prw_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200001010000-200012312100_polar_nearest.nc\n",
    "\n",
    "# #create the map file\n",
    "# ESMF_RegridWeightGen --ignore_unmapped --src_regional -m neareststod -w map_${srcgrid}_to_${dstgrid}_near.nc -s ${srcgridfile} -d ${dstgridfile}\n",
    "\n",
    "# #use the mapfile to remap srcinitfile to dstinitfile\n",
    "# ncremap -m ./map_${srcgrid}_to_${dstgrid}_near.nc -i ${srcinitfile} -o ${dstinitfile}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2ba3c5-f12e-4c61-9c95-366cf5f41fd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Rename regridded files to avoid lat/lon being dimension and variable name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0d1fd4-f6aa-48d0-a53d-7717a02fdd9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use ncrename to rename the output lat/lon dims to not conflict with vars\n",
    "# ncrename -v lon,longitude -d lat,latitude prw_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200001010000-200012312100.nc renamed/prw_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200001010000-200012312100.nc\n",
    "\n",
    "pr_ds = xr.open_dataset('/glade/scratch/tking/cgnet/high_lat_QC/prw/2dlatlon/polar_tmq/renamed/renamed_2/prw_A3hr_CAM5-1-025degree_All-Hist_est1_v1-0_run002_200001010000-200012312100.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1727102c-4a99-493c-90e5-357e27d12d63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pr_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc325c77-d889-4a5f-ae6d-8fe5c2ad9e53",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate combined mask/underlying data files by creating netcdf from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13434f1-62c3-4439-a89c-49a0a30734c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### run the next cell if temp.nc (temporary file to fill) already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ab9c5e-4bd1-4047-8514-f572ce8fa4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm /glade/u/home/tking/work/cgnet/QA_xml/all_antarctic_converted_masks/temp.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12cef01-3e47-42cc-95cb-0a0843bf9123",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Note on time adjustment\n",
    "We'll need to use the bug fix included in the below cells for the first two rounds of QC'd data; this issue has been fixed in the masks of following datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b365e3d0-5f24-4271-a9b1-adb4a3b06ce6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create temp file with correct attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3923a380-aa50-490b-b80a-90139416b44b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_file = '/glade/work/tking/cgnet/polar_regridding/data-2003-04-29-02-0-sav1-rev-latlon.nc'\n",
    "temp = xr.open_dataset(temp_file)\n",
    "\n",
    "ncfile = Dataset('/glade/u/home/tking/work/cgnet/QA_xml/all_antarctic_converted_masks/temp.nc',mode='w',format='NETCDF4_CLASSIC') \n",
    "\n",
    "# Create dimensions\n",
    "y_dim = ncfile.createDimension('y', 1152)        # vertical displacement axis\n",
    "x_dim = ncfile.createDimension('x', 1152)        # horizontal displacement axis\n",
    "time_dim = ncfile.createDimension('time', None)  # unlimited axis (can be appended to)\n",
    "sample_id_dim = ncfile.createDimension('sample_id', 6)\n",
    "\n",
    "# Include time variable and relevant attributes\n",
    "time = ncfile.createVariable('time', np.float64, ('time',))\n",
    "time.units = 'hours since 1970-01-01'\n",
    "time.calendar = 'noleap'\n",
    "time.long_name = 'time'\n",
    "\n",
    "# Include date information\n",
    "date = ncfile.createVariable('date', np.float64, ('time',))\n",
    "date.long_name = 'current date'\n",
    "datesec = ncfile.createVariable('datesec', np.float64, ('time',))\n",
    "datesec.long_name = 'current seconds of current date'\n",
    "\n",
    "# Include ar_mask variable and relevant attributes\n",
    "ar_mask = ncfile.createVariable('ar_mask', np.float64, ('time','sample_id','y','x'))\n",
    "ar_mask.long_name = \"Atmospheric River Mask\"\n",
    "ar_mask.standard_name = \"AR flag\"\n",
    "ar_mask.flag_values = 0, 1\n",
    "ar_mask.flag_meanings = \"Background Atmospheric_River\"\n",
    "\n",
    "# Include underlying data\n",
    "tmq = ncfile.createVariable('tmq',np.float64,('time','y','x'))\n",
    "ivt = ncfile.createVariable('ivt',np.float64,('time','y','x'))\n",
    "psl = ncfile.createVariable('psl',np.float64,('time','y','x'))\n",
    "pr = ncfile.createVariable('pr',np.float64,('time','y','x'))\n",
    "\n",
    "# include y, x, lat, & lon from temp file\n",
    "y = ncfile.createVariable('y',np.float64,('y'))\n",
    "y.long_name = 'vertical offset from pole'\n",
    "y.units = 'meters'\n",
    "\n",
    "x = ncfile.createVariable('x',np.float64,('x'))\n",
    "x.long_name = 'horizontal offset from pole'\n",
    "x.units = 'meters'\n",
    "\n",
    "lat = ncfile.createVariable('lat', np.float64,('y','x'))\n",
    "lat.units = 'degrees_north'\n",
    "lat.long_name = 'latitude'\n",
    "\n",
    "lon = ncfile.createVariable('lon', np.float64,('y','x'))\n",
    "lon.units = 'degrees_east'\n",
    "lon.long_name = 'longitude'\n",
    "\n",
    "# Add the y, x, lat, & lon data values to the netcdf file\n",
    "ncfile['y'][:] = temp.y\n",
    "ncfile['x'][:] = temp.x\n",
    "ncfile['lat'][:,:] = temp.lat\n",
    "ncfile['lon'][:,:] = temp.lon\n",
    "\n",
    "# Copy temp file metadata\n",
    "ivt.transform = temp.__xarray_dataarray_variable__.transform\n",
    "ivt.crs = temp.__xarray_dataarray_variable__.crs\n",
    "ivt.coordinates = 'lat lon'\n",
    "tmq.transform = temp.__xarray_dataarray_variable__.transform\n",
    "tmq.crs = temp.__xarray_dataarray_variable__.crs\n",
    "tmq.coordinates = 'lat lon'\n",
    "psl.transform = temp.__xarray_dataarray_variable__.transform\n",
    "psl.crs = temp.__xarray_dataarray_variable__.crs\n",
    "psl.coordinates = 'lat lon'\n",
    "pr.transform = temp.__xarray_dataarray_variable__.transform\n",
    "pr.crs = temp.__xarray_dataarray_variable__.crs\n",
    "pr.coordinates = 'lat lon'\n",
    "\n",
    "# Copy global metadata\n",
    "ncfile.transform = temp.__xarray_dataarray_variable__.transform\n",
    "ncfile.crs = temp.__xarray_dataarray_variable__.crs\n",
    "ncfile.res = temp.__xarray_dataarray_variable__.res\n",
    "ncfile.nodatavals = temp.__xarray_dataarray_variable__.nodatavals\n",
    "ncfile.scales = temp.__xarray_dataarray_variable__.scales\n",
    "ncfile.offsets = temp.__xarray_dataarray_variable__.offsets\n",
    "ncfile.AREA_OR_POINT = temp.__xarray_dataarray_variable__.AREA_OR_POINT\n",
    "ncfile.coordinates = \"lat lon\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de80de7-0a4a-4ab0-a0e0-911d4e8fd284",
   "metadata": {},
   "source": [
    "### Loop through mask files and underlying data files; add both to temp file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5922839e-e5e5-4d66-a8ed-81c2195808b0",
   "metadata": {},
   "source": [
    "My process has been to adjust the year in the for loop, move temp.nc to a new name, check the results, and then run for a different year. One could also rename temp.nc above to correspond with the year and then not bother with renaming the files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be517570-9b0c-4f87-9f0b-07de5e9378c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('starting at {}'.format(dt.datetime.now()))\n",
    "directory_of_underlying_data = \"/glade/derecho/scratch/tking/cgnet/high_lat_QC/from_nersc/2dlatlon/polar/renamed/\"\n",
    "time_index = -1\n",
    "round_val = 3 # change this value to indicate whether or not bug fix from round 1 is used\n",
    "\n",
    "# The years below correspond to the mask's listed years (ie, incorrect years from round 1)\n",
    "# For processing data, I recommend running one year at a time and then renaming temp.nc to match whatever that year is\n",
    "for year in [2000]:\n",
    "    # gather data from a particular round of QC\n",
    "    # some of these required different data processing steps which are adjusted based on the round_val below\n",
    "    if round_val == 1:\n",
    "        mask_file_list = sorted(glob.glob('/glade/work/tking/cgnet/QA_xml/round_1/h5/qa*/antarctic/netcdfs/data-{}-*'.format(year)))\n",
    "    elif round_val == 2:\n",
    "        mask_file_list = sorted(glob.glob('/glade/work/tking/cgnet/QA_xml/round_2/h5/qa*/antarctic/netcdfs/data-{}-*'.format(year)))\n",
    "    elif round_val == 3:\n",
    "        mask_file_list = sorted(glob.glob('/glade/work/tking/cgnet/QA_xml/round_3/h5/qa*/antarctic/netcdfs/data-{}-*'.format(year)))\n",
    "    \n",
    "    if round_val == 1:\n",
    "        shifted_year = year - 1\n",
    "    else:\n",
    "        shifted_year = year\n",
    "    \n",
    "    # Bug fix from 2000 data being pulled in previously:\n",
    "    if round_val == 1:\n",
    "        shifted_year = 2000\n",
    "\n",
    "    # get underlying data\n",
    "    tmq_ds = xr.open_dataset(directory_of_underlying_data+'tmq/{}'.format(tmq_dict[shifted_year]))\n",
    "    psl_ds = xr.open_dataset(directory_of_underlying_data+'psl/{}'.format(psl_dict[shifted_year]))\n",
    "    ivt_ds = xr.open_dataset(directory_of_underlying_data+'ivt/{}'.format(ivt_dict[shifted_year]))\n",
    "    pr_ds = xr.open_dataset(directory_of_underlying_data+'pr/{}'.format(pr_dict[shifted_year]))\n",
    "    \n",
    "    # loop through mask files, get corresponding underlying data, and add to temporary file\n",
    "    for antarctic_mask_file in mask_file_list[:]:\n",
    "        time = antarctic_mask_file.split('/')[-1].split('data-')[1].split('.nc')[0].split('-00-2')[0].split('_')[0]\n",
    "        time_year = int(time.split('-')[0])  # or = year\n",
    "                \n",
    "        # For round 1, assume that the nc file is ~named~ 2001, but underlying data is 2000\n",
    "        if round_val == 1:\n",
    "            time_year = 2001\n",
    "        \n",
    "        time_month = int(time.split('-')[1])\n",
    "        time_day = int(time.split('-')[2])\n",
    "        if round_val==1 or round_val==2:\n",
    "            time_hour = 22  # all files were 00\n",
    "            time_mins = 30\n",
    "        if round_val==3:\n",
    "            time_hour = int(time.split('-')[4])*3-2  # 1.5 hours off of time provided... so subtract 2 hr and add a half hour\n",
    "            time_mins = 30\n",
    "            if time_year == 2000:\n",
    "                if time_month >= 3: # Account for leap day in march and later months for year 2000\n",
    "                    # if time_day < 31 or 30:\n",
    "                    time_day = time_day+1\n",
    "                    # if time_day==31 or 30:\n",
    "                    #     time_day = 1\n",
    "                    #     time_month = time_month +1\n",
    "                    # we would need a slightly more robust way of doing this if there were masks incorrectly labelled from the last day of the month,\n",
    "                    #     but this works for now\n",
    "\n",
    "        date_number = date2num(dt.datetime(time_year, time_month, time_day, time_hour, time_mins), 'hours since 1970-01-01')\n",
    "\n",
    "        # ---------------------------------------------------------------------------------\n",
    "        # Fix for indexing bugs is now in all_chey_arctic.ipynb and all_chey_antarctic.ipynb,\n",
    "        # So this fix will not be needed after round 1 data is processed.\n",
    "        # In this fix, if data is from 2000, adjust days by 4, otherwise, adjust days by 5 (to account for leap days)\n",
    "        if round_val == 1:\n",
    "            if time_year == 2000:\n",
    "                leap_year_adjustment = 4\n",
    "            if time_year >= 2001:\n",
    "                leap_year_adjustment = 5\n",
    "            if time_month in [1, 3, 5, 7, 8, 10, 12]: # months with 31 days\n",
    "                days_in_month = 31\n",
    "                # adjust year for indexing issue unless last few days in file (because of leap year, these got included in correct file)\n",
    "                if time_month == 12 and time_day < (days_in_month - leap_year_adjustment):\n",
    "                    time_year = time_year - 1\n",
    "                else:\n",
    "                    time_year = time_year - 1\n",
    "                if time_day < (days_in_month - leap_year_adjustment):\n",
    "                    time_day = time_day + leap_year_adjustment\n",
    "                else:\n",
    "                    time_day = ((time_day + leap_year_adjustment) - days_in_month) + 1 # add 4 days for leap year, subtract days in month, add 1 because month starts at 1 not 0.\n",
    "                    time_month += 1 # use one of the first few days in the next month\n",
    "                    if time_month == 13:  # no 13th month, so loop back to January\n",
    "                        time_month = 1\n",
    "            elif time_month in [4, 6, 9, 11]: # months with 30 days\n",
    "                time_year = time_year - 1\n",
    "                days_in_month = 30\n",
    "                if time_day < (days_in_month - leap_year_adjustment):\n",
    "                    time_day = time_day + leap_year_adjustment\n",
    "                else:\n",
    "                    time_day = ((time_day + leap_year_adjustment) - days_in_month) + 1\n",
    "                    time_month += 1 # use one of the first few days in the next month\n",
    "            elif time_month == 2:\n",
    "                time_year = time_year - 1\n",
    "                days_in_month = 28\n",
    "                if time_day < (days_in_month - leap_year_adjustment):\n",
    "                    time_day = time_day + leap_year_adjustment\n",
    "                else:\n",
    "                    time_day = ((time_day + leap_year_adjustment) - days_in_month) + 1\n",
    "                    time_month += 1 # use one of the first few days in the next month\n",
    "        # ---------------------------------------------------------------------------------\n",
    "\n",
    "        # format strings for use in netcdf date attribute\n",
    "        if time_month < 10:\n",
    "            time_m_formatted = '0'+str(time_month)\n",
    "        else:\n",
    "            time_m_formatted = str(time_month)\n",
    "        if time_day < 10:\n",
    "            time_d_formatted = '0'+str(time_day)\n",
    "        else:\n",
    "            time_d_formatted = str(time_day)\n",
    "\n",
    "        # Some files (mostly 2002) start at _0 and sometimes just end with -2 and if repeat date, include _1.nc\n",
    "        # only increase time index if moving to a new time; otherwise additional masks can be in a new sample_id of the same time index\n",
    "        if antarctic_mask_file[-5:-4] == '_':\n",
    "            sample_id = int(antarctic_mask_file[-4:-3])\n",
    "        else:\n",
    "            sample_id = 0\n",
    "\n",
    "        if sample_id == 0:\n",
    "            time_index+=1\n",
    "        \n",
    "        qa_aa_ds = xr.open_dataset(antarctic_mask_file)\n",
    "        \n",
    "        # reorientation needed for viewing with matplotlib\n",
    "        qa_aa_ds.ar_masks['phony_dim_0'] = qa_aa_ds.ar_masks['phony_dim_0'][::-1]\n",
    "        qa_aa_ds.reindex(phony_dim_0=list(reversed(qa_aa_ds.phony_dim_0)))\n",
    "        \n",
    "        # fill in date and datesec\n",
    "        date[time_index] = str(time_year)+time_m_formatted+time_d_formatted\n",
    "        if round_val==1 or round_val==2:\n",
    "            datesec[time_index] = '81000'  # corresponds to 22:30\n",
    "        if round_val==3:\n",
    "            datesec[time_index] = str((3600*time_hour)+(60*time_mins))\n",
    "        # fill in ar_mask\n",
    "        ar_mask[time_index,sample_id,:,:]=qa_aa_ds.ar_masks\n",
    "        # get data arrays for underlying data\n",
    "        pr = xr.DataArray(pr)\n",
    "        psl = xr.DataArray(psl)\n",
    "        tmq = xr.DataArray(tmq)\n",
    "        ivt = xr.DataArray(ivt)\n",
    "\n",
    "        # fill in underlying data parts of temporary ncfile\n",
    "        if round_val==1 or round_val ==2:\n",
    "            ncfile['pr'][time_index,:,:] = pr_ds.sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True), method='nearest').isel(height=0).to_array().dropna('variable').dropna('nvertices').dropna('nb2')[5,:,:,1,1]\n",
    "            ncfile['psl'][time_index,:,:] = psl_ds.sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True), method='nearest').to_array().dropna('variable').dropna('nvertices').dropna('nbnd')[5,:,:,1,1]\n",
    "            ncfile['tmq'][time_index,:,:] = tmq_ds.sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True), method='nearest').to_array().dropna('variable').dropna('nvertices').dropna('nbnd')[5,:,:,1,1]\n",
    "            ncfile['ivt'][time_index,:,:] = ivt_ds.sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True), method='nearest').to_array().dropna('variable').dropna('nvertices').dropna('bound')[7,:,:,1,1]\n",
    "        if round_val==3:\n",
    "            ncfile['pr'][time_index,:,:] = pr_ds.sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True), method='nearest').isel(height=0).to_array().dropna('variable').dropna('nvertices').dropna('nb2')[3,:,:,1,1]\n",
    "            ncfile['psl'][time_index,:,:] = psl_ds.sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True), method='nearest').to_array().dropna('variable').dropna('nvertices').dropna('nbnd')[3,:,:,1,1]\n",
    "            ncfile['tmq'][time_index,:,:] = tmq_ds.sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True), method='nearest').to_array().dropna('variable').dropna('nvertices').dropna('nbnd')[3,:,:,1,1]\n",
    "            ncfile['ivt'][time_index,:,:] = ivt_ds.sel(time=cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True), method='nearest').to_array().dropna('variable').dropna('nvertices').dropna('bound')[6,:,:,1,1]\n",
    "            \n",
    "        print('all data added for {}'.format(str(time_year)+time_m_formatted+time_d_formatted))\n",
    "\n",
    "        # time reported on netcdf frame:\n",
    "        time_val = cftime.DatetimeNoLeap(time_year, time_month, time_day, time_hour, time_mins, 0, 0, has_year_zero=True) - cftime.DatetimeNoLeap(1970, 1, 1, 0, 0, 0, 0, has_year_zero=True)\n",
    "        ncfile['time'][time_index] = (time_val.days * 24) + (time_val.seconds / 3600)\n",
    "    # write netcdf\n",
    "    ncfile.close()\n",
    "print('wrote netcdf at {}'.format(dt.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45661f34-c4f5-4fda-a128-5a23216f0cbe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PART 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3db2ed-6193-4f7a-a3f7-03c044a8b6bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Note: have not actually used this in the data processing\n",
    "\n",
    "### use the standard ESMF mapping procedure to go from the projected stereographic polar mask grids to our regular gridded data from CESM. Use Steve's python code and John's mapping file commands to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45921ba-158d-41b6-9e24-2267d97f81d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to do this to get masks and state information on two different grids\n",
    "# need new mapping file to do this, then run through Steve's routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1afaf7-eca7-4d74-9d1e-3ead5749189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steve Yeager has a utility function for remapping CAM-SE output (see remap_camse function below):\n",
    "#     https://github.com/sgyeager/mypyutils/blob/main/mypyutils/regrid_utils.py\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "import cf_xarray\n",
    "\n",
    "def remap_camse(ds, dsw, varlst=[]):\n",
    "    #dso = xr.full_like(ds.drop_dims('ncol'), np.nan)\n",
    "    dso = ds.drop_dims('ncol').copy()\n",
    "    lonb = dsw.xc_b.values.reshape([dsw.dst_grid_dims[1].values, dsw.dst_grid_dims[0].values])\n",
    "    latb = dsw.yc_b.values.reshape([dsw.dst_grid_dims[1].values, dsw.dst_grid_dims[0].values])\n",
    "    weights = sps.coo_matrix((dsw.S, (dsw.row-1, dsw.col-1)), shape=[dsw.dims['n_b'], dsw.dims['n_a']])\n",
    "    if not varlst:\n",
    "        for varname in list(ds):\n",
    "            if 'ncol' in(ds[varname].dims):\n",
    "                varlst.append(varname)\n",
    "        if 'lon' in varlst: varlst.remove('lon')\n",
    "        if 'lat' in varlst: varlst.remove('lat')\n",
    "        if 'area' in varlst: varlst.remove('area')\n",
    "    for varname in varlst:\n",
    "        shape = ds[varname].shape\n",
    "        invar_flat = ds[varname].values.reshape(-1, shape[-1])\n",
    "        remapped_flat = weights.dot(invar_flat.T).T\n",
    "        remapped = remapped_flat.reshape([*shape[0:-1], dsw.dst_grid_dims[1].values,\n",
    "                                          dsw.dst_grid_dims[0].values])\n",
    "        dimlst = list(ds[varname].dims[0:-1])\n",
    "        dims={}\n",
    "        coords={}\n",
    "        for it in dimlst:\n",
    "            dims[it] = dso.dims[it]\n",
    "            coords[it] = dso.coords[it]\n",
    "        dims['lat'] = int(dsw.dst_grid_dims[1])\n",
    "        dims['lon'] = int(dsw.dst_grid_dims[0])\n",
    "        coords['lat'] = latb[:,0]\n",
    "        coords['lon'] = lonb[0,:]\n",
    "        remapped = xr.DataArray(remapped, coords=coords, dims=dims, attrs=ds[varname].attrs)\n",
    "        dso = xr.merge([dso, remapped.to_dataset(name=varname)])\n",
    "    return dso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487ba097-5da5-44b2-a2f2-e9e6f79f2c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a notebook demonstrating how this is used:\n",
    "#     /glade/u/home/yeager/analysis/python/toshare/CLM_field_regrid.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4178e0c9-fac9-49a8-b017-9dbc0a00d4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PolarARs",
   "language": "python",
   "name": "polarars"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
